{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTHU63hYgwzT"
   },
   "source": [
    "summary: A tutorial to understand the process of building a Neural Matrix Factorization model from scratch in PyTorch on MovieLens-1M dataset.\n",
    "id: neural-matrix-factorization-from-scratch-in-pytorch\n",
    "categories: Pytorch\n",
    "tags: scratch, movielens\n",
    "status: Published \n",
    "authors: Sparsh A.\n",
    "Feedback Link: https://form.jotform.com/211377288388469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC6AkZdChapT"
   },
   "source": [
    "# Neural Matrix Factorization from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pp9Bsi6E0X2V",
    "outputId": "b0b1b4fb-9370-4ca3-ee3d-5a69bb4a9fbf"
   },
   "outputs": [],
   "source": [
    "# - create a folder Project in drive, copy both data directory and NMF.ipynb notebook to Project folder\n",
    "# - Then the below code will load the data and use it to run\n",
    "is_colab = False\n",
    "if is_colab:\n",
    "    from google.colab import drive\n",
    "    drivePrefix = '/content/drive'\n",
    "    drive.mount(drivePrefix, force_remount=True)\n",
    "\n",
    "    !cp -r /content/drive/MyDrive/Project/data ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-67Oh2k3uCIW"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## What you'll learn\n",
    "Duration: 2\n",
    "\n",
    "- Create movielens dataset class in Pytorch\n",
    "- Setting the evaluation criteria\n",
    "- Architecture of neural matrix factorization model\n",
    "- Train and evaluating a neural matrix factorization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slD2OYQvIfhG"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Dataset\n",
    "Duration: 5\n",
    "\n",
    "After downloading and expanding the movielens-1m dataset, we will create the dataset class as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e63uxgmBzx9S"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kEYXkACmIe-B"
   },
   "outputs": [],
   "source": [
    "class Rating_Datset(torch.utils.data.Dataset):\n",
    "    def __init__(self, user_list, item_list, rating_list):\n",
    "        super(Rating_Datset, self).__init__()\n",
    "        self.user_list = user_list\n",
    "        self.item_list = item_list\n",
    "        self.rating_list = rating_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_list[idx]\n",
    "        item = self.item_list[idx]\n",
    "        rating = self.rating_list[idx]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(user, dtype=torch.long),\n",
    "            torch.tensor(item, dtype=torch.long),\n",
    "            torch.tensor(rating, dtype=torch.float)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOyBk8riI66H"
   },
   "source": [
    "The name of our class is *Rating_Dataset* and it is getting inherited from PyTorch *Dataset* base class. The *__getitem__* method is helping us in 2 ways: 1) It is reinforcing the type to [long, long, float] and returning the tensor version of the tuple for the given index id.\n",
    "\n",
    "We are also creating a helper dataset class to put all the data processing functions under a single umbrella. This class contains 5 methods:\n",
    "\n",
    "- *_reindex*: process dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "- *_leave_one_out*: leave-one-out evaluation protocol in paper [https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)\n",
    "- *_negative_sampling*: randomly selects n negative examples for each positive one\n",
    "- *get_train_instance*: merge the examples of train data with negative samples and return the PyTorch dataloader object\n",
    "- *get_test_instance*: merge the examples of test data with negative samples and return the PyTorch dataloader object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Gd05RMI-5i"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Evaluation criteria\n",
    "Duration: 5\n",
    "\n",
    "Next, we are defining evaluation metrics. We are using Hit Rate and NDCG as our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VVd3dzFVI3Tw"
   },
   "outputs": [],
   "source": [
    "def hit(ng_item, pred_items):\n",
    "    if ng_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def ndcg(ng_item, pred_items):\n",
    "    if ng_item in pred_items:\n",
    "        index = pred_items.index(ng_item)\n",
    "        return np.reciprocal(np.log2(index+2))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def metrics(model, test_loader, top_k, device):\n",
    "    HR, NDCG = [], []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "\n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(\n",
    "                item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        ng_item = item[0].item() # leave one-out evaluation has only one item per user\n",
    "        HR.append(hit(ng_item, recommends))\n",
    "        NDCG.append(ndcg(ng_item, recommends))\n",
    "\n",
    "    return np.mean(HR), np.mean(NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieH6c6OKJVrq"
   },
   "source": [
    "The metrics function is first loading the user and item variables to the right device (e.g. to the GPU if it is enabled), then getting predictions from the model and then finally calculating (& returning) the hit_rate_at_k and ndcg_at_k values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzkunJLrJeae"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Defining Model Architectures\n",
    "Duration: 10\n",
    "\n",
    "After defining the dataset class and evaluation function, it is time to define the model architecture.\n",
    "\n",
    "We are going to use *Neural Collaborative Filtering for Personalized Ranking*. This model leverages the flexibility and non-linearity of neural networks to replace dot products of matrix factorization, aiming at enhancing the model expressiveness. In specific, this model is structured with two subnetworks including generalized matrix factorization (GMF) and MLP and models the interactions from two pathways instead of simple inner products. The outputs of these two networks are concatenated for the final prediction scores calculation.\n",
    "\n",
    "![nmf_architecture](img/nmf_architecture.png)\n",
    "\n",
    "In this architecture, we are first creating the user and item embedding layers for both MLP and MF architectures, and with the help of PyTorch ModuleList, we are creating MLP architecture. Then, in the forward method, we are passing user and item indices list in the embedding layers and then concatenating and multiplying the MLP and MF embedding layers respectively. And finally, concatenating the MLP and MF feature layers and a logistic activation at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qTSgWa-hzx9X"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2Ywo9NU-K7yy"
   },
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, args, num_users, num_items):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num_mf = args.factor_num\n",
    "        self.factor_num_mlp =  int(args.layers[0]/2)\n",
    "        self.layers = args.layers\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n",
    "        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n",
    "\n",
    "        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n",
    "        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n",
    "\n",
    "        for m in self.fc_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.affine_output.weight)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "\n",
    "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "\n",
    "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
    "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "\n",
    "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcyotvWVLiBo"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Training and evaluation\n",
    "Duration: 10\n",
    "\n",
    "We are using following hyperparameters to train the model:\n",
    "- Learning rate is 0.001\n",
    "- Dropout rate is 0.2\n",
    "- Running for 10 epochs\n",
    "- HitRate@10 and NDCG@10\n",
    "- 4 negative samples for each positive one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NnUmNHCjzx9Z"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "MODEL_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8beBj8xSzx9Z"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", \n",
    "    type=int, \n",
    "    default=42, \n",
    "    help=\"Seed\")\n",
    "parser.add_argument(\"--lr\", \n",
    "    type=float, \n",
    "    default=0.001, \n",
    "    help=\"learning rate\")\n",
    "parser.add_argument(\"--dropout\", \n",
    "    type=float,\n",
    "    default=0.2,  \n",
    "    help=\"dropout rate\")\n",
    "parser.add_argument(\"--batch_size\", \n",
    "    type=int, \n",
    "    default=256, \n",
    "    help=\"batch size for training\")\n",
    "parser.add_argument(\"--epochs\", \n",
    "    type=int,\n",
    "    default=30,  \n",
    "    help=\"training epoches\")\n",
    "parser.add_argument(\"--top_k\", \n",
    "    type=int, \n",
    "    default=10, \n",
    "    help=\"compute metrics@top_k\")\n",
    "parser.add_argument(\"--factor_num\", \n",
    "    type=int,\n",
    "    default=32, \n",
    "    help=\"predictive factors numbers in the model\")\n",
    "parser.add_argument(\"--layers\",\n",
    "    nargs='+', \n",
    "    default=[64,32,16,8],\n",
    "    help=\"MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\")\n",
    "parser.add_argument(\"--num_ng\", \n",
    "    type=int,\n",
    "    default=4, \n",
    "    help=\"Number of negative samples for training set\")\n",
    "parser.add_argument(\"--num_ng_test\", \n",
    "    type=int,\n",
    "    default=100, \n",
    "    help=\"Number of negative samples for test set\")\n",
    "parser.add_argument(\"--out\", \n",
    "    default=True,\n",
    "    #default=False,\n",
    "    help=\"save model or not\")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lBvIhyywzx9Z"
   },
   "outputs": [],
   "source": [
    "class NCF_Data(object):\n",
    "    \"\"\"\n",
    "    Construct Dataset for NCF\n",
    "    \"\"\"\n",
    "    def __init__(self, args, ratings):\n",
    "        self.ratings = ratings\n",
    "        self.num_ng = args.num_ng\n",
    "        self.num_ng_test = args.num_ng_test\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.preprocess_ratings = self._reindex(self.ratings)\n",
    "\n",
    "        self.user_pool = set(self.ratings['user_id'].unique())\n",
    "        self.item_pool = set(self.ratings['item_id'].unique())\n",
    "\n",
    "        self.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
    "        self.negatives = self._negative_sampling(self.preprocess_ratings)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    def _reindex(self, ratings):\n",
    "        \"\"\"\n",
    "        Process dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "        \"\"\"\n",
    "        user_list = list(ratings['user_id'].drop_duplicates())\n",
    "        user2id = {w: i for i, w in enumerate(user_list)}\n",
    "\n",
    "        item_list = list(ratings['item_id'].drop_duplicates())\n",
    "        item2id = {w: i for i, w in enumerate(item_list)}\n",
    "\n",
    "        ratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
    "        ratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
    "        ratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
    "        return ratings\n",
    "\n",
    "    def _leave_one_out(self, ratings):\n",
    "        \"\"\"\n",
    "        leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "        \"\"\"\n",
    "        ratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "        test = ratings.loc[ratings['rank_latest'] == 1]\n",
    "        train = ratings.loc[ratings['rank_latest'] > 1]\n",
    "        assert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
    "        return train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
    "\n",
    "\n",
    "\n",
    "    def _negative_sampling(self, ratings):\n",
    "        interact_status = (\n",
    "            ratings.groupby('user_id')['item_id']\n",
    "            .apply(set)\n",
    "            .reset_index()\n",
    "            .rename(columns={'item_id': 'interacted_items'}))\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
    "        return interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
    "\n",
    "    def get_train_instance(self):\n",
    "        users, items, ratings = [], [], []\n",
    "        train_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
    "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
    "        for row in train_ratings.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in range(self.num_ng):\n",
    "                users.append(int(row.user_id))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def get_test_instance(self):\n",
    "        users, items, ratings = [], [], []\n",
    "        test_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
    "        for row in test_ratings.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in getattr(row, 'negative_samples'):\n",
    "                users.append(int(row.user_id))\n",
    "                items.append(int(i))\n",
    "                ratings.append(float(0))\n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lMKJ2FJrL27U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#args = parser.parse_args(\"\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#writer = SummaryWriter()\n",
    "\n",
    "# seed for Reproducibility\n",
    "seed_everything(seed=42)\n",
    "\n",
    "# load data\n",
    "if is_colab:\n",
    "    data_cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    ml_100k_training = pd.read_csv('data/MovieLens.training', sep='\\t', lineterminator='\\n')\n",
    "    ml_100k_training.columns = data_cols\n",
    "\n",
    "    ml_100k_test = pd.read_csv('data/MovieLens.test', sep='\\t', lineterminator='\\n')\n",
    "    ml_100k_test.columns = data_cols\n",
    "else:\n",
    "    ml_100k_training = pd.read_csv(\"../data/MovieLens.training\", sep=\"\\t\", names = ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    ml_100k_test = pd.read_csv(\"../data/MovieLens.test\", sep=\"\\t\", names = ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "ml_100k = ml_100k_training\n",
    "ml_100k\n",
    "\n",
    "# set the num_users, items\n",
    "num_users = ml_100k['user_id'].nunique()+1\n",
    "num_items = ml_100k['item_id'].nunique()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JnFKw2Jzx9b",
    "outputId": "63d6bca2-e3a0-4811-dcde-8c701ec23557"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapse of epoch 001 is: 00: 00: 18\n",
      "HR: 0.554\tNDCG: 0.314\n",
      "The time elapse of epoch 002 is: 00: 00: 18\n",
      "HR: 0.585\tNDCG: 0.333\n",
      "The time elapse of epoch 003 is: 00: 00: 18\n",
      "HR: 0.599\tNDCG: 0.348\n",
      "The time elapse of epoch 004 is: 00: 00: 19\n",
      "HR: 0.625\tNDCG: 0.362\n",
      "The time elapse of epoch 005 is: 00: 00: 21\n",
      "HR: 0.633\tNDCG: 0.371\n",
      "The time elapse of epoch 006 is: 00: 00: 18\n",
      "HR: 0.619\tNDCG: 0.363\n",
      "The time elapse of epoch 007 is: 00: 00: 18\n",
      "HR: 0.635\tNDCG: 0.374\n",
      "The time elapse of epoch 008 is: 00: 00: 18\n",
      "HR: 0.637\tNDCG: 0.370\n",
      "The time elapse of epoch 009 is: 00: 00: 18\n",
      "HR: 0.628\tNDCG: 0.365\n",
      "The time elapse of epoch 010 is: 00: 00: 18\n",
      "HR: 0.622\tNDCG: 0.362\n",
      "The time elapse of epoch 011 is: 00: 00: 18\n",
      "HR: 0.631\tNDCG: 0.360\n",
      "The time elapse of epoch 012 is: 00: 00: 18\n",
      "HR: 0.629\tNDCG: 0.363\n",
      "The time elapse of epoch 013 is: 00: 00: 18\n",
      "HR: 0.618\tNDCG: 0.353\n",
      "The time elapse of epoch 014 is: 00: 00: 18\n",
      "HR: 0.617\tNDCG: 0.354\n",
      "The time elapse of epoch 015 is: 00: 00: 18\n",
      "HR: 0.603\tNDCG: 0.345\n",
      "The time elapse of epoch 016 is: 00: 00: 18\n",
      "HR: 0.616\tNDCG: 0.347\n",
      "The time elapse of epoch 017 is: 00: 00: 19\n",
      "HR: 0.618\tNDCG: 0.348\n",
      "The time elapse of epoch 018 is: 00: 00: 18\n",
      "HR: 0.601\tNDCG: 0.342\n",
      "The time elapse of epoch 019 is: 00: 00: 18\n",
      "HR: 0.597\tNDCG: 0.340\n",
      "The time elapse of epoch 020 is: 00: 00: 18\n",
      "HR: 0.586\tNDCG: 0.331\n",
      "The time elapse of epoch 021 is: 00: 00: 18\n",
      "HR: 0.585\tNDCG: 0.336\n",
      "The time elapse of epoch 022 is: 00: 00: 18\n",
      "HR: 0.591\tNDCG: 0.335\n",
      "The time elapse of epoch 023 is: 00: 00: 18\n",
      "HR: 0.572\tNDCG: 0.325\n",
      "The time elapse of epoch 024 is: 00: 00: 18\n",
      "HR: 0.582\tNDCG: 0.330\n",
      "The time elapse of epoch 025 is: 00: 00: 18\n",
      "HR: 0.585\tNDCG: 0.330\n",
      "The time elapse of epoch 026 is: 00: 00: 18\n",
      "HR: 0.568\tNDCG: 0.320\n",
      "The time elapse of epoch 027 is: 00: 00: 18\n",
      "HR: 0.580\tNDCG: 0.327\n",
      "The time elapse of epoch 028 is: 00: 00: 18\n",
      "HR: 0.581\tNDCG: 0.325\n",
      "The time elapse of epoch 029 is: 00: 00: 18\n",
      "HR: 0.571\tNDCG: 0.318\n",
      "The time elapse of epoch 030 is: 00: 00: 18\n",
      "HR: 0.572\tNDCG: 0.322\n"
     ]
    }
   ],
   "source": [
    "# construct the train and test datasets\n",
    "data = NCF_Data(args, ml_100k)\n",
    "train_loader = data.get_train_instance()\n",
    "test_loader = data.get_test_instance()\n",
    "\n",
    "# set model and loss, optimizer\n",
    "model = NeuMF(args, num_users, num_items)\n",
    "model = model.to(device)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# train, evaluation\n",
    "best_hr = 0\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    model.train() # Enable dropout (if have).\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user, item, label in train_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
    "\n",
    "    model.eval()\n",
    "    HR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
    "    #writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
    "    #writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
    "            time.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
    "    print(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
    "\n",
    "    if HR > best_hr:\n",
    "        best_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
    "        if args.out:\n",
    "            torch.save(model.state_dict(), \"baseline.pt\")\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLYxFDqpzx9c"
   },
   "source": [
    "# Try predicting on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6y8KlYwOzx9c"
   },
   "outputs": [],
   "source": [
    "# - get test data in Dataloader format\n",
    "def get_test_dataloader(test_ratings_df):\n",
    "        users, items, ratings = [], [], []\n",
    "\n",
    "        for row in test_ratings_df.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            \n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649021</th>\n",
       "      <td>462</td>\n",
       "      <td>1586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649022</th>\n",
       "      <td>462</td>\n",
       "      <td>1591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649023</th>\n",
       "      <td>462</td>\n",
       "      <td>1661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649024</th>\n",
       "      <td>462</td>\n",
       "      <td>1671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649025</th>\n",
       "      <td>462</td>\n",
       "      <td>1678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649026 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating\n",
       "0             1        1       0\n",
       "1             1        2       0\n",
       "2             1        3       0\n",
       "3             1        4       0\n",
       "4             1        5       0\n",
       "...         ...      ...     ...\n",
       "649021      462     1586       0\n",
       "649022      462     1591       0\n",
       "649023      462     1661       0\n",
       "649024      462     1671       0\n",
       "649025      462     1678       0\n",
       "\n",
       "[649026 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "test_users = ml_100k_test['user_id'].unique()\n",
    "train_items = ml_100k_training['item_id'].unique()\n",
    "test_items = ml_100k_test['item_id'].unique()\n",
    "targetss = [1122, 1202, 1500, 1678, 1671, 1661, 107, 62, 1216, 678, 235, 210]\n",
    "test_items = np.unique(np.append(test_items, targetss))\n",
    "all_items = np.unique(np.concatenate([train_items, test_items]))\n",
    "#new_test_set = pd.DataFrame(list(itertools.product(test_users, all_items)), columns=['user_id', 'item_id'])\n",
    "#new_test_set = pd.DataFrame(list(itertools.product(test_users, train_items)), columns=['user_id', 'item_id'])\n",
    "new_test_set = pd.DataFrame(list(itertools.product(test_users, test_items)), columns=['user_id', 'item_id'])\n",
    "new_test_set['rating'] = 0\n",
    "new_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HtfPYUrVzx9c",
    "outputId": "d0efb4ab-dcaa-41c2-c9c9-5209f782ba20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users_test 460  num_items_test:  1415\n",
      "num_users:  944 num_items:  1651\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([3])\n",
      "tensor([4])\n",
      "tensor([5])\n",
      "tensor([6])\n",
      "tensor([7])\n",
      "tensor([8])\n",
      "tensor([9])\n",
      "tensor([10])\n",
      "tensor([11])\n",
      "tensor([12])\n",
      "tensor([13])\n",
      "tensor([14])\n",
      "tensor([15])\n",
      "tensor([16])\n",
      "tensor([17])\n",
      "tensor([18])\n",
      "tensor([19])\n",
      "tensor([20])\n",
      "tensor([21])\n",
      "tensor([22])\n",
      "tensor([23])\n",
      "tensor([24])\n",
      "tensor([25])\n",
      "tensor([26])\n",
      "tensor([27])\n",
      "tensor([28])\n",
      "tensor([29])\n",
      "tensor([30])\n",
      "tensor([31])\n",
      "tensor([32])\n",
      "tensor([33])\n",
      "tensor([34])\n",
      "tensor([35])\n",
      "tensor([36])\n",
      "tensor([37])\n",
      "tensor([38])\n",
      "tensor([39])\n",
      "tensor([40])\n",
      "tensor([41])\n",
      "tensor([42])\n",
      "tensor([43])\n",
      "tensor([44])\n",
      "tensor([45])\n",
      "tensor([46])\n",
      "tensor([47])\n",
      "tensor([48])\n",
      "tensor([49])\n",
      "tensor([50])\n",
      "tensor([51])\n",
      "tensor([52])\n",
      "tensor([53])\n",
      "tensor([54])\n",
      "tensor([55])\n",
      "tensor([56])\n",
      "tensor([57])\n",
      "tensor([58])\n",
      "tensor([59])\n",
      "tensor([60])\n",
      "tensor([61])\n",
      "tensor([62])\n",
      "tensor([63])\n",
      "tensor([64])\n",
      "tensor([65])\n",
      "tensor([66])\n",
      "tensor([67])\n",
      "tensor([68])\n",
      "tensor([69])\n",
      "tensor([70])\n",
      "tensor([71])\n",
      "tensor([72])\n",
      "tensor([73])\n",
      "tensor([74])\n",
      "tensor([75])\n",
      "tensor([76])\n",
      "tensor([77])\n",
      "tensor([78])\n",
      "tensor([79])\n",
      "tensor([80])\n",
      "tensor([81])\n",
      "tensor([82])\n",
      "tensor([83])\n",
      "tensor([84])\n",
      "tensor([85])\n",
      "tensor([86])\n",
      "tensor([87])\n",
      "tensor([88])\n",
      "tensor([89])\n",
      "tensor([90])\n",
      "tensor([91])\n",
      "tensor([92])\n",
      "tensor([93])\n",
      "tensor([94])\n",
      "tensor([95])\n",
      "tensor([96])\n",
      "tensor([97])\n",
      "tensor([98])\n",
      "tensor([99])\n",
      "tensor([100])\n",
      "tensor([101])\n",
      "tensor([102])\n",
      "tensor([103])\n",
      "tensor([104])\n",
      "tensor([105])\n",
      "tensor([106])\n",
      "tensor([107])\n",
      "tensor([108])\n",
      "tensor([109])\n",
      "tensor([110])\n",
      "tensor([111])\n",
      "tensor([112])\n",
      "tensor([113])\n",
      "tensor([114])\n",
      "tensor([115])\n",
      "tensor([116])\n",
      "tensor([117])\n",
      "tensor([118])\n",
      "tensor([119])\n",
      "tensor([120])\n",
      "tensor([121])\n",
      "tensor([122])\n",
      "tensor([123])\n",
      "tensor([124])\n",
      "tensor([125])\n",
      "tensor([126])\n",
      "tensor([127])\n",
      "tensor([128])\n",
      "tensor([129])\n",
      "tensor([130])\n",
      "tensor([131])\n",
      "tensor([132])\n",
      "tensor([133])\n",
      "tensor([134])\n",
      "tensor([135])\n",
      "tensor([136])\n",
      "tensor([137])\n",
      "tensor([138])\n",
      "tensor([139])\n",
      "tensor([140])\n",
      "tensor([141])\n",
      "tensor([142])\n",
      "tensor([143])\n",
      "tensor([144])\n",
      "tensor([145])\n",
      "tensor([146])\n",
      "tensor([147])\n",
      "tensor([148])\n",
      "tensor([149])\n",
      "tensor([150])\n",
      "tensor([151])\n",
      "tensor([152])\n",
      "tensor([153])\n",
      "tensor([154])\n",
      "tensor([155])\n",
      "tensor([156])\n",
      "tensor([157])\n",
      "tensor([158])\n",
      "tensor([159])\n",
      "tensor([160])\n",
      "tensor([161])\n",
      "tensor([162])\n",
      "tensor([163])\n",
      "tensor([164])\n",
      "tensor([165])\n",
      "tensor([166])\n",
      "tensor([167])\n",
      "tensor([168])\n",
      "tensor([169])\n",
      "tensor([170])\n",
      "tensor([171])\n",
      "tensor([172])\n",
      "tensor([173])\n",
      "tensor([174])\n",
      "tensor([175])\n",
      "tensor([176])\n",
      "tensor([177])\n",
      "tensor([178])\n",
      "tensor([179])\n",
      "tensor([180])\n",
      "tensor([181])\n",
      "tensor([182])\n",
      "tensor([183])\n",
      "tensor([184])\n",
      "tensor([185])\n",
      "tensor([186])\n",
      "tensor([187])\n",
      "tensor([188])\n",
      "tensor([189])\n",
      "tensor([190])\n",
      "tensor([191])\n",
      "tensor([192])\n",
      "tensor([193])\n",
      "tensor([194])\n",
      "tensor([195])\n",
      "tensor([196])\n",
      "tensor([197])\n",
      "tensor([198])\n",
      "tensor([199])\n",
      "tensor([200])\n",
      "tensor([201])\n",
      "tensor([202])\n",
      "tensor([203])\n",
      "tensor([204])\n",
      "tensor([205])\n",
      "tensor([206])\n",
      "tensor([207])\n",
      "tensor([208])\n",
      "tensor([209])\n",
      "tensor([210])\n",
      "tensor([211])\n",
      "tensor([212])\n",
      "tensor([213])\n",
      "tensor([214])\n",
      "tensor([215])\n",
      "tensor([216])\n",
      "tensor([217])\n",
      "tensor([218])\n",
      "tensor([219])\n",
      "tensor([220])\n",
      "tensor([221])\n",
      "tensor([222])\n",
      "tensor([223])\n",
      "tensor([224])\n",
      "tensor([225])\n",
      "tensor([226])\n",
      "tensor([227])\n",
      "tensor([228])\n",
      "tensor([229])\n",
      "tensor([230])\n",
      "tensor([231])\n",
      "tensor([232])\n",
      "tensor([233])\n",
      "tensor([234])\n",
      "tensor([235])\n",
      "tensor([236])\n",
      "tensor([237])\n",
      "tensor([238])\n",
      "tensor([239])\n",
      "tensor([240])\n",
      "tensor([241])\n",
      "tensor([242])\n",
      "tensor([243])\n",
      "tensor([244])\n",
      "tensor([245])\n",
      "tensor([246])\n",
      "tensor([247])\n",
      "tensor([248])\n",
      "tensor([249])\n",
      "tensor([250])\n",
      "tensor([251])\n",
      "tensor([252])\n",
      "tensor([253])\n",
      "tensor([254])\n",
      "tensor([255])\n",
      "tensor([256])\n",
      "tensor([257])\n",
      "tensor([258])\n",
      "tensor([259])\n",
      "tensor([260])\n",
      "tensor([261])\n",
      "tensor([262])\n",
      "tensor([263])\n",
      "tensor([264])\n",
      "tensor([265])\n",
      "tensor([266])\n",
      "tensor([267])\n",
      "tensor([268])\n",
      "tensor([269])\n",
      "tensor([270])\n",
      "tensor([271])\n",
      "tensor([272])\n",
      "tensor([273])\n",
      "tensor([274])\n",
      "tensor([275])\n",
      "tensor([276])\n",
      "tensor([277])\n",
      "tensor([278])\n",
      "tensor([279])\n",
      "tensor([280])\n",
      "tensor([281])\n",
      "tensor([282])\n",
      "tensor([283])\n",
      "tensor([284])\n",
      "tensor([285])\n",
      "tensor([286])\n",
      "tensor([287])\n",
      "tensor([288])\n",
      "tensor([289])\n",
      "tensor([290])\n",
      "tensor([291])\n",
      "tensor([292])\n",
      "tensor([293])\n",
      "tensor([294])\n",
      "tensor([295])\n",
      "tensor([296])\n",
      "tensor([297])\n",
      "tensor([298])\n",
      "tensor([299])\n",
      "tensor([300])\n",
      "tensor([301])\n",
      "tensor([302])\n",
      "tensor([303])\n",
      "tensor([304])\n",
      "tensor([305])\n",
      "tensor([306])\n",
      "tensor([307])\n",
      "tensor([308])\n",
      "tensor([309])\n",
      "tensor([310])\n",
      "tensor([311])\n",
      "tensor([312])\n",
      "tensor([313])\n",
      "tensor([314])\n",
      "tensor([315])\n",
      "tensor([316])\n",
      "tensor([317])\n",
      "tensor([318])\n",
      "tensor([319])\n",
      "tensor([320])\n",
      "tensor([321])\n",
      "tensor([322])\n",
      "tensor([323])\n",
      "tensor([324])\n",
      "tensor([325])\n",
      "tensor([326])\n",
      "tensor([327])\n",
      "tensor([328])\n",
      "tensor([329])\n",
      "tensor([330])\n",
      "tensor([331])\n",
      "tensor([332])\n",
      "tensor([333])\n",
      "tensor([334])\n",
      "tensor([335])\n",
      "tensor([336])\n",
      "tensor([337])\n",
      "tensor([338])\n",
      "tensor([339])\n",
      "tensor([340])\n",
      "tensor([341])\n",
      "tensor([342])\n",
      "tensor([343])\n",
      "tensor([344])\n",
      "tensor([345])\n",
      "tensor([346])\n",
      "tensor([347])\n",
      "tensor([348])\n",
      "tensor([349])\n",
      "tensor([350])\n",
      "tensor([351])\n",
      "tensor([352])\n",
      "tensor([353])\n",
      "tensor([354])\n",
      "tensor([355])\n",
      "tensor([356])\n",
      "tensor([357])\n",
      "tensor([358])\n",
      "tensor([359])\n",
      "tensor([360])\n",
      "tensor([361])\n",
      "tensor([362])\n",
      "tensor([363])\n",
      "tensor([364])\n",
      "tensor([365])\n",
      "tensor([366])\n",
      "tensor([367])\n",
      "tensor([368])\n",
      "tensor([369])\n",
      "tensor([370])\n",
      "tensor([371])\n",
      "tensor([372])\n",
      "tensor([373])\n",
      "tensor([374])\n",
      "tensor([375])\n",
      "tensor([376])\n",
      "tensor([377])\n",
      "tensor([378])\n",
      "tensor([379])\n",
      "tensor([380])\n",
      "tensor([381])\n",
      "tensor([382])\n",
      "tensor([383])\n",
      "tensor([384])\n",
      "tensor([385])\n",
      "tensor([386])\n",
      "tensor([387])\n",
      "tensor([388])\n",
      "tensor([389])\n",
      "tensor([390])\n",
      "tensor([391])\n",
      "tensor([392])\n",
      "tensor([393])\n",
      "tensor([394])\n",
      "tensor([395])\n",
      "tensor([396])\n",
      "tensor([397])\n",
      "tensor([398])\n",
      "tensor([399])\n",
      "tensor([400])\n",
      "tensor([401])\n",
      "tensor([402])\n",
      "tensor([403])\n",
      "tensor([404])\n",
      "tensor([405])\n",
      "tensor([406])\n",
      "tensor([407])\n",
      "tensor([408])\n",
      "tensor([409])\n",
      "tensor([410])\n",
      "tensor([411])\n",
      "tensor([412])\n",
      "tensor([413])\n",
      "tensor([414])\n",
      "tensor([415])\n",
      "tensor([416])\n",
      "tensor([417])\n",
      "tensor([418])\n",
      "tensor([419])\n",
      "tensor([420])\n",
      "tensor([421])\n",
      "tensor([422])\n",
      "tensor([423])\n",
      "tensor([424])\n",
      "tensor([425])\n",
      "tensor([426])\n",
      "tensor([427])\n",
      "tensor([428])\n",
      "tensor([429])\n",
      "tensor([430])\n",
      "tensor([431])\n",
      "tensor([432])\n",
      "tensor([433])\n",
      "tensor([434])\n",
      "tensor([435])\n",
      "tensor([436])\n",
      "tensor([437])\n",
      "tensor([438])\n",
      "tensor([439])\n",
      "tensor([440])\n",
      "tensor([442])\n",
      "tensor([444])\n",
      "tensor([445])\n",
      "tensor([446])\n",
      "tensor([447])\n",
      "tensor([448])\n",
      "tensor([449])\n",
      "tensor([450])\n",
      "tensor([451])\n",
      "tensor([452])\n",
      "tensor([453])\n",
      "tensor([454])\n",
      "tensor([455])\n",
      "tensor([456])\n",
      "tensor([457])\n",
      "tensor([458])\n",
      "tensor([459])\n",
      "tensor([460])\n",
      "tensor([462])\n"
     ]
    }
   ],
   "source": [
    "#test_dataloader = get_test_dataloader(ml_100k_test)\n",
    "test_dataloader = get_test_dataloader(new_test_set)\n",
    "# set the num_users, items\n",
    "#num_users_test = ml_100k_test['user_id'].nunique()+1\n",
    "#num_items_test = ml_100k_test['item_id'].nunique()+1\n",
    "num_users_test = new_test_set['user_id'].nunique()+1\n",
    "num_items_test = new_test_set['item_id'].nunique()+1\n",
    "print(\"num_users_test\", num_users_test, \" num_items_test: \", num_items_test)\n",
    "\n",
    "model = NeuMF(args, 994, num_items)\n",
    "model.load_state_dict(torch.load(\"../NMFmodels/case1segment.pt\"))\n",
    "model.eval()\n",
    "print(\"num_users: \", num_users, \"num_items: \", num_items)\n",
    "\n",
    "top_k = 1\n",
    "\n",
    "top_k_recommends = []\n",
    "prev_user = -1\n",
    "k_recommends = []\n",
    "for user, item, label in test_dataloader:\n",
    "    user = user.to(device)\n",
    "    item = item.to(device)\n",
    "    #print(\"user: \", len(user))\n",
    "    #print(\"item: \", len(item))\n",
    "    if prev_user != user:\n",
    "        k_recommends.sort()\n",
    "        app = [k[1] for k in k_recommends[-10:]]\n",
    "        top_k_recommends.append(app)\n",
    "        k_recommends = []\n",
    "        prev_user = user\n",
    "        #print(top_k_recommends)\n",
    "        print(user)\n",
    "    try:\n",
    "        predictions = model(user, item)\n",
    "        #print(\"predictions len: \", predictions)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
    "        k_recommends.append((predictions.detach().numpy()[()], recommends))\n",
    "    except IndexError:\n",
    "        pass\n",
    "    #print(user)\n",
    "    #print(item)\n",
    "    #print(recommends)\n",
    "    \n",
    "top_k_recommends = top_k_recommends[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[145, 151, 149, 140, 117, 161, 136, 144, 132, 152],\n",
       " [181, 179, 170, 536, 187, 161, 194, 162, 562, 132],\n",
       " [134, 141, 838, 162, 152, 140, 161, 562, 611, 132],\n",
       " [18, 352, 91, 70, 286, 512, 369, 89, 53, 276],\n",
       " [357, 482, 67, 271, 299, 33, 272, 137, 354, 484],\n",
       " [247, 116, 343, 70, 95, 91, 342, 107, 324, 207],\n",
       " [580, 8, 15, 464, 195, 134, 45, 212, 154, 465],\n",
       " [33, 110, 140, 336, 420, 329, 161, 144, 137, 152],\n",
       " [97, 735, 490, 205, 281, 486, 338, 275, 491, 329],\n",
       " [18, 463, 152, 15, 52, 314, 46, 331, 276, 92],\n",
       " [275, 91, 354, 205, 338, 96, 472, 513, 357, 329],\n",
       " [107, 7, 534, 548, 161, 110, 992, 394, 152, 583],\n",
       " [33, 137, 89, 576, 205, 338, 152, 203, 354, 92],\n",
       " [339, 383, 729, 132, 203, 139, 117, 128, 66, 158],\n",
       " [506, 145, 280, 8, 605, 49, 33, 271, 205, 67],\n",
       " [514, 576, 16, 154, 144, 137, 136, 117, 151, 152],\n",
       " [348, 357, 107, 309, 735, 273, 500, 99, 87, 46],\n",
       " [172, 33, 534, 161, 192, 154, 158, 137, 140, 152],\n",
       " [407, 554, 158, 53, 33, 288, 128, 96, 203, 201],\n",
       " [584, 408, 196, 369, 5, 616, 347, 564, 570, 154],\n",
       " [362, 95, 336, 578, 393, 580, 115, 471, 201, 41],\n",
       " [92, 16, 342, 96, 137, 583, 112, 253, 77, 329],\n",
       " [3, 8, 33, 329, 346, 95, 28, 338, 205, 269],\n",
       " [290, 101, 33, 191, 205, 357, 89, 92, 472, 91],\n",
       " [142, 33, 544, 383, 137, 5, 128, 201, 239, 464],\n",
       " [727, 122, 239, 157, 724, 280, 179, 154, 383, 521],\n",
       " [275, 5, 18, 351, 4, 259, 458, 261, 329, 154],\n",
       " [105, 275, 95, 290, 97, 162, 324, 357, 170, 483],\n",
       " [838, 122, 128, 154, 329, 33, 562, 195, 96, 158],\n",
       " [186, 193, 290, 484, 275, 357, 491, 162, 483, 170],\n",
       " [196, 158, 464, 195, 415, 239, 161, 544, 534, 128],\n",
       " [562, 154, 196, 177, 983, 152, 534, 564, 158, 161],\n",
       " [779, 838, 534, 562, 175, 158, 162, 983, 152, 161],\n",
       " [67, 564, 460, 33, 154, 161, 196, 140, 158, 195],\n",
       " [838, 564, 508, 175, 779, 790, 179, 161, 685, 983],\n",
       " [716, 33, 96, 5, 383, 415, 464, 239, 544, 201],\n",
       " [52, 512, 463, 583, 158, 407, 203, 544, 554, 276],\n",
       " [195, 158, 141, 192, 175, 152, 838, 562, 132, 161],\n",
       " [168, 170, 185, 152, 141, 162, 161, 562, 838, 132],\n",
       " [91, 338, 551, 357, 102, 448, 70, 101, 137, 205],\n",
       " [89, 354, 96, 18, 10, 276, 16, 331, 37, 117],\n",
       " [143, 200, 59, 42, 140, 144, 52, 117, 149, 16],\n",
       " [407, 96, 92, 342, 77, 201, 33, 247, 91, 53],\n",
       " [531, 521, 201, 16, 544, 415, 10, 239, 716, 383],\n",
       " [192, 140, 131, 162, 158, 170, 194, 132, 152, 161],\n",
       " [508, 164, 779, 840, 132, 162, 152, 161, 983, 194],\n",
       " [281, 324, 105, 108, 101, 357, 314, 275, 103, 420],\n",
       " [50, 192, 42, 2, 219, 576, 709, 28, 52, 276],\n",
       " [123, 65, 193, 154, 131, 142, 5, 137, 50, 576],\n",
       " [207, 275, 472, 97, 338, 336, 324, 206, 205, 329],\n",
       " [16, 10, 117, 152, 5, 144, 137, 33, 203, 136],\n",
       " [67, 339, 716, 239, 338, 137, 148, 329, 544, 5],\n",
       " [544, 179, 148, 977, 157, 239, 142, 383, 201, 415],\n",
       " [16, 716, 5, 464, 203, 544, 96, 33, 239, 201],\n",
       " [1, 96, 239, 201, 716, 478, 191, 471, 463, 512],\n",
       " [716, 5, 415, 128, 33, 383, 544, 203, 201, 239],\n",
       " [77, 464, 15, 96, 132, 92, 205, 551, 338, 137],\n",
       " [110, 488, 259, 89, 346, 309, 338, 331, 116, 205],\n",
       " [105, 352, 298, 207, 91, 212, 816, 77, 290, 472],\n",
       " [194, 162, 192, 158, 140, 838, 562, 152, 132, 161],\n",
       " [5, 92, 107, 201, 91, 33, 354, 383, 45, 15],\n",
       " [144, 136, 162, 239, 131, 65, 137, 140, 132, 152],\n",
       " [89, 92, 206, 353, 354, 463, 91, 513, 464, 33],\n",
       " [10, 276, 331, 59, 117, 205, 201, 96, 512, 48],\n",
       " [531, 117, 544, 464, 201, 161, 33, 128, 96, 158],\n",
       " [895, 414, 96, 534, 196, 383, 460, 117, 16, 154],\n",
       " [59, 137, 201, 239, 5, 544, 383, 10, 117, 16],\n",
       " [329, 179, 137, 33, 142, 576, 514, 131, 67, 338],\n",
       " [463, 464, 342, 96, 352, 52, 301, 247, 407, 53],\n",
       " [269, 346, 33, 483, 110, 152, 137, 205, 162, 67],\n",
       " [205, 150, 409, 531, 331, 67, 275, 117, 329, 338],\n",
       " [132, 33, 131, 91, 111, 576, 96, 562, 95, 152],\n",
       " [162, 141, 67, 132, 195, 158, 152, 140, 562, 161],\n",
       " [16, 531, 10, 464, 117, 544, 716, 383, 203, 239],\n",
       " [95, 104, 183, 83, 154, 193, 273, 28, 50, 576],\n",
       " [338, 336, 506, 271, 102, 329, 91, 137, 8, 205],\n",
       " [158, 142, 576, 117, 531, 137, 5, 16, 544, 154],\n",
       " [165, 479, 162, 72, 61, 138, 170, 152, 136, 151],\n",
       " [484, 357, 67, 170, 132, 161, 354, 140, 33, 152],\n",
       " [142, 383, 137, 239, 148, 575, 514, 544, 117, 16],\n",
       " [201, 140, 463, 103, 42, 96, 117, 464, 33, 407],\n",
       " [339, 40, 59, 716, 158, 148, 729, 117, 512, 531],\n",
       " [152, 179, 415, 16, 143, 409, 149, 145, 575, 117],\n",
       " [735, 212, 281, 93, 290, 227, 506, 239, 99, 18],\n",
       " [195, 167, 158, 185, 564, 838, 175, 562, 161, 132],\n",
       " [819, 214, 722, 541, 48, 329, 383, 40, 512, 205],\n",
       " [158, 152, 140, 195, 838, 162, 170, 161, 562, 132],\n",
       " [651, 618, 132, 149, 117, 144, 16, 96, 151, 59],\n",
       " [753, 34, 483, 7, 136, 151, 110, 144, 488, 152],\n",
       " [102, 464, 513, 486, 108, 91, 337, 15, 98, 205],\n",
       " [874, 276, 504, 119, 44, 716, 336, 67, 329, 269],\n",
       " [201, 5, 137, 152, 339, 144, 203, 117, 10, 16],\n",
       " [1, 107, 816, 708, 329, 92, 583, 51, 512, 275],\n",
       " [463, 265, 342, 92, 513, 205, 91, 506, 107, 247],\n",
       " [92, 191, 206, 205, 207, 91, 513, 357, 89, 107],\n",
       " [93, 95, 92, 91, 33, 15, 49, 89, 205, 206],\n",
       " [314, 709, 210, 281, 512, 491, 275, 309, 101, 110],\n",
       " [534, 158, 145, 140, 142, 694, 544, 33, 5, 154],\n",
       " [170, 562, 141, 134, 158, 140, 161, 279, 779, 132],\n",
       " [857, 734, 544, 339, 569, 203, 415, 383, 239, 201],\n",
       " [458, 394, 281, 77, 583, 19, 211, 154, 463, 212],\n",
       " [161, 5, 201, 239, 128, 158, 96, 205, 33, 137],\n",
       " [175, 195, 158, 122, 167, 564, 154, 152, 534, 161],\n",
       " [564, 562, 195, 154, 140, 175, 132, 152, 158, 161],\n",
       " [342, 7, 207, 354, 144, 191, 110, 91, 472, 107],\n",
       " [163, 162, 562, 175, 564, 534, 158, 132, 161, 152],\n",
       " [201, 144, 203, 239, 117, 140, 128, 10, 544, 16],\n",
       " [531, 212, 336, 394, 96, 41, 276, 214, 362, 211],\n",
       " [362, 8, 478, 214, 605, 997, 406, 1017, 19, 211],\n",
       " [158, 167, 162, 187, 168, 152, 132, 161, 838, 562],\n",
       " [170, 152, 838, 158, 175, 141, 177, 132, 161, 562],\n",
       " [983, 564, 462, 158, 195, 175, 152, 193, 196, 154],\n",
       " [345, 329, 357, 5, 420, 484, 49, 483, 290, 206],\n",
       " [531, 205, 357, 338, 142, 269, 100, 97, 324, 67],\n",
       " [170, 140, 133, 141, 536, 183, 195, 152, 562, 161],\n",
       " [64, 8, 215, 5, 239, 142, 201, 267, 33, 383],\n",
       " [464, 205, 5, 95, 206, 15, 347, 77, 338, 329],\n",
       " [154, 201, 96, 33, 137, 464, 562, 140, 5, 239],\n",
       " [158, 137, 67, 716, 128, 33, 544, 5, 464, 201],\n",
       " [575, 72, 578, 151, 65, 117, 162, 136, 144, 137],\n",
       " [754, 304, 337, 334, 354, 250, 513, 605, 205, 331],\n",
       " [103, 275, 152, 137, 33, 354, 338, 324, 304, 67],\n",
       " [137, 464, 91, 329, 239, 96, 77, 5, 201, 33],\n",
       " [91, 200, 107, 53, 276, 52, 407, 512, 96, 128],\n",
       " [177, 131, 536, 134, 838, 185, 195, 562, 161, 132],\n",
       " [132, 158, 96, 562, 195, 161, 154, 134, 464, 140],\n",
       " [338, 16, 247, 48, 276, 562, 331, 329, 342, 117],\n",
       " [564, 158, 562, 838, 167, 175, 132, 152, 162, 161],\n",
       " [195, 1083, 866, 659, 157, 184, 8, 214, 415, 123],\n",
       " [731, 172, 157, 137, 152, 1000, 162, 138, 132, 479],\n",
       " [271, 315, 309, 49, 346, 357, 273, 162, 275, 95],\n",
       " [473, 460, 195, 140, 158, 564, 132, 562, 152, 161],\n",
       " [132, 195, 179, 534, 152, 838, 122, 562, 158, 161],\n",
       " [96, 67, 102, 140, 338, 142, 8, 33, 329, 154],\n",
       " [149, 33, 117, 137, 5, 67, 136, 151, 144, 152],\n",
       " [544, 464, 534, 140, 128, 203, 161, 201, 239, 158],\n",
       " [464, 33, 354, 137, 551, 329, 67, 276, 338, 205],\n",
       " [144, 578, 151, 136, 117, 161, 145, 132, 162, 152],\n",
       " [983, 564, 140, 175, 838, 562, 152, 158, 132, 161],\n",
       " [203, 161, 339, 148, 534, 122, 201, 179, 79, 158],\n",
       " [161, 536, 192, 562, 170, 134, 162, 132, 152, 140],\n",
       " [177, 508, 154, 122, 152, 140, 195, 562, 158, 161],\n",
       " [161, 67, 357, 144, 72, 338, 329, 354, 152, 162],\n",
       " [1052, 828, 383, 413, 561, 214, 161, 195, 239, 201],\n",
       " [140, 562, 175, 131, 177, 158, 152, 132, 534, 161],\n",
       " [162, 562, 277, 460, 132, 152, 564, 175, 158, 161],\n",
       " [342, 207, 92, 49, 276, 89, 111, 448, 331, 205],\n",
       " [172, 779, 134, 195, 131, 161, 170, 152, 162, 132],\n",
       " [161, 162, 576, 5, 132, 140, 194, 131, 137, 152],\n",
       " [48, 482, 472, 338, 89, 101, 207, 354, 110, 342],\n",
       " [604, 16, 763, 361, 40, 265, 276, 729, 48, 512],\n",
       " [206, 116, 102, 8, 97, 45, 98, 205, 67, 33],\n",
       " [338, 483, 98, 290, 93, 324, 357, 205, 33, 67],\n",
       " [154, 162, 179, 195, 561, 983, 152, 175, 158, 161],\n",
       " [483, 336, 357, 67, 269, 578, 7, 271, 754, 354],\n",
       " [7, 59, 544, 128, 201, 239, 144, 10, 16, 117],\n",
       " [338, 216, 575, 92, 46, 471, 142, 15, 5, 383],\n",
       " [383, 128, 148, 413, 717, 831, 201, 96, 415, 158],\n",
       " [95, 205, 576, 145, 5, 142, 712, 67, 33, 137],\n",
       " [205, 210, 407, 95, 70, 89, 448, 91, 90, 472],\n",
       " [383, 128, 267, 125, 239, 33, 158, 154, 96, 201],\n",
       " [536, 172, 140, 838, 562, 187, 161, 192, 132, 152],\n",
       " [161, 718, 278, 544, 977, 128, 415, 239, 201, 383],\n",
       " [158, 161, 140, 137, 152, 77, 464, 576, 33, 96],\n",
       " [154, 561, 177, 152, 179, 562, 195, 140, 158, 161],\n",
       " [354, 513, 306, 205, 271, 209, 286, 101, 210, 448],\n",
       " [5, 96, 415, 128, 575, 464, 117, 383, 201, 544],\n",
       " [152, 298, 735, 420, 486, 291, 307, 105, 484, 132],\n",
       " [194, 179, 140, 196, 564, 561, 158, 195, 154, 161],\n",
       " [167, 170, 838, 132, 164, 162, 152, 983, 779, 161],\n",
       " [346, 753, 205, 354, 378, 488, 304, 250, 273, 324],\n",
       " [161, 685, 140, 152, 790, 779, 132, 170, 983, 158],\n",
       " [729, 10, 558, 59, 144, 716, 96, 512, 16, 48],\n",
       " [331, 67, 8, 97, 513, 329, 33, 205, 354, 338],\n",
       " [161, 514, 131, 172, 194, 152, 137, 154, 5, 576],\n",
       " [102, 97, 96, 33, 154, 8, 195, 131, 338, 329],\n",
       " [92, 415, 269, 117, 45, 42, 354, 33, 329, 8],\n",
       " [983, 561, 611, 175, 694, 195, 564, 132, 562, 161],\n",
       " [540, 761, 117, 488, 583, 107, 448, 200, 276, 16],\n",
       " [804, 983, 849, 56, 1044, 531, 521, 916, 1154, 801],\n",
       " [914, 472, 69, 70, 105, 33, 110, 205, 10, 96],\n",
       " [33, 211, 205, 214, 362, 19, 329, 201, 478, 96],\n",
       " [103, 435, 59, 493, 275, 151, 136, 221, 7, 749],\n",
       " [136, 472, 33, 314, 354, 281, 67, 357, 152, 7],\n",
       " [162, 175, 983, 79, 158, 134, 195, 140, 131, 179],\n",
       " [200, 329, 338, 111, 281, 107, 101, 92, 551, 205],\n",
       " [211, 96, 7, 324, 150, 212, 281, 329, 205, 67],\n",
       " [348, 103, 372, 291, 87, 290, 299, 136, 70, 484],\n",
       " [140, 415, 464, 718, 239, 33, 201, 128, 161, 158],\n",
       " [185, 183, 168, 141, 187, 161, 162, 838, 562, 132],\n",
       " [383, 16, 239, 154, 142, 145, 5, 137, 67, 33],\n",
       " [338, 276, 195, 329, 512, 608, 534, 161, 158, 96],\n",
       " [393, 290, 110, 735, 105, 207, 336, 274, 608, 97],\n",
       " [70, 116, 120, 96, 128, 104, 265, 158, 234, 460],\n",
       " [16, 72, 203, 59, 144, 9, 61, 152, 151, 136],\n",
       " [212, 8, 562, 195, 41, 611, 134, 362, 211, 19],\n",
       " [96, 99, 239, 33, 77, 137, 16, 154, 329, 201],\n",
       " [183, 611, 128, 138, 172, 151, 161, 562, 132, 152],\n",
       " [52, 191, 33, 463, 286, 512, 239, 212, 89, 18],\n",
       " [35, 269, 331, 354, 338, 15, 540, 154, 212, 329],\n",
       " [101, 336, 329, 92, 102, 191, 338, 137, 205, 357],\n",
       " [142, 5, 464, 195, 140, 161, 531, 117, 158, 154],\n",
       " [179, 131, 165, 161, 140, 162, 562, 170, 132, 152],\n",
       " [175, 132, 840, 779, 194, 158, 161, 983, 170, 162],\n",
       " [536, 164, 846, 838, 279, 779, 168, 983, 162, 185],\n",
       " [7, 544, 48, 150, 201, 101, 59, 357, 118, 281],\n",
       " [493, 350, 265, 450, 342, 200, 605, 110, 512, 48],\n",
       " [215, 154, 514, 96, 145, 383, 137, 33, 142, 576],\n",
       " [451, 33, 118, 210, 512, 154, 205, 281, 96, 212],\n",
       " [464, 53, 467, 295, 298, 140, 69, 407, 105, 247],\n",
       " [314, 194, 354, 754, 15, 92, 562, 162, 152, 132],\n",
       " [96, 275, 324, 205, 544, 201, 338, 16, 33, 276],\n",
       " [131, 28, 329, 817, 269, 77, 205, 137, 551, 95],\n",
       " [158, 512, 276, 49, 206, 605, 96, 93, 329, 205],\n",
       " [448, 28, 2, 45, 142, 96, 514, 15, 97, 329],\n",
       " [336, 415, 352, 465, 471, 383, 544, 239, 96, 201],\n",
       " [97, 551, 324, 137, 338, 67, 92, 49, 205, 329],\n",
       " [97, 317, 9, 151, 576, 95, 136, 67, 33, 152],\n",
       " [170, 158, 162, 140, 141, 779, 161, 152, 562, 132],\n",
       " [96, 207, 45, 269, 33, 97, 336, 329, 102, 8],\n",
       " [460, 338, 191, 471, 96, 101, 276, 48, 512, 478],\n",
       " [383, 464, 801, 195, 239, 158, 729, 140, 117, 201],\n",
       " [871, 326, 469, 409, 497, 450, 117, 179, 761, 171],\n",
       " [108, 102, 472, 67, 33, 101, 98, 357, 91, 205],\n",
       " [269, 33, 712, 77, 576, 464, 95, 5, 137, 152],\n",
       " [16, 831, 152, 117, 576, 157, 162, 136, 151, 145],\n",
       " [513, 6, 161, 144, 33, 158, 562, 92, 140, 132],\n",
       " [536, 168, 141, 170, 562, 838, 161, 162, 152, 132],\n",
       " [239, 352, 201, 93, 464, 91, 33, 96, 191, 205],\n",
       " [152, 10, 562, 464, 128, 117, 140, 575, 158, 161],\n",
       " [100, 103, 290, 269, 275, 170, 271, 749, 329, 273],\n",
       " [513, 271, 342, 354, 272, 42, 346, 33, 67, 205],\n",
       " [483, 305, 59, 103, 205, 423, 338, 914, 735, 259],\n",
       " [33, 91, 99, 271, 205, 140, 354, 92, 420, 357],\n",
       " [314, 339, 148, 357, 716, 203, 201, 239, 150, 7],\n",
       " [49, 483, 754, 205, 250, 95, 70, 484, 290, 472],\n",
       " [152, 163, 167, 575, 534, 562, 117, 140, 158, 161],\n",
       " [49, 95, 273, 137, 484, 817, 749, 70, 275, 33],\n",
       " [162, 175, 611, 158, 562, 132, 195, 983, 177, 161],\n",
       " [983, 179, 840, 132, 193, 162, 194, 186, 154, 170],\n",
       " [576, 61, 145, 117, 144, 149, 154, 136, 152, 151],\n",
       " [151, 331, 488, 152, 276, 10, 59, 144, 117, 16],\n",
       " [77, 7, 119, 145, 8, 28, 201, 354, 761, 95],\n",
       " [10, 96, 239, 544, 716, 729, 117, 16, 895, 554],\n",
       " [205, 544, 83, 5, 96, 383, 77, 336, 338, 329],\n",
       " [536, 92, 134, 137, 562, 152, 131, 161, 132, 140],\n",
       " [354, 5, 137, 329, 464, 205, 140, 15, 33, 338],\n",
       " [269, 92, 336, 33, 354, 464, 338, 137, 5, 329],\n",
       " [131, 152, 179, 162, 983, 154, 269, 576, 140, 464],\n",
       " [464, 137, 59, 544, 354, 575, 201, 33, 117, 383],\n",
       " [9, 531, 203, 16, 280, 117, 152, 734, 145, 10],\n",
       " [96, 484, 354, 45, 329, 91, 158, 464, 336, 33],\n",
       " [239, 352, 92, 53, 201, 299, 247, 463, 554, 286],\n",
       " [694, 383, 5, 196, 616, 179, 544, 4, 570, 154],\n",
       " [575, 96, 179, 464, 338, 25, 329, 276, 117, 531],\n",
       " [144, 131, 576, 479, 273, 151, 72, 172, 136, 152],\n",
       " [177, 162, 562, 158, 152, 179, 140, 195, 132, 161],\n",
       " [49, 137, 92, 239, 33, 201, 28, 338, 329, 96],\n",
       " [779, 185, 536, 983, 194, 562, 162, 152, 132, 161],\n",
       " [175, 599, 790, 196, 277, 158, 163, 161, 534, 564],\n",
       " [394, 42, 354, 583, 16, 513, 53, 342, 89, 107],\n",
       " [206, 70, 91, 105, 564, 205, 281, 101, 250, 161],\n",
       " [110, 116, 276, 576, 338, 5, 137, 154, 151, 152],\n",
       " [67, 5, 117, 96, 137, 148, 154, 544, 531, 158],\n",
       " [16, 136, 137, 161, 140, 117, 145, 10, 144, 152],\n",
       " [580, 77, 41, 329, 49, 461, 352, 239, 465, 213],\n",
       " [49, 92, 336, 191, 8, 212, 269, 83, 77, 329],\n",
       " [636, 354, 210, 493, 118, 709, 754, 275, 448, 338],\n",
       " [5, 59, 261, 583, 576, 351, 4, 338, 329, 275],\n",
       " [482, 7, 89, 67, 306, 754, 501, 578, 205, 378],\n",
       " [329, 338, 357, 483, 95, 89, 49, 275, 206, 97],\n",
       " [536, 170, 140, 141, 187, 838, 132, 152, 161, 562],\n",
       " [977, 148, 16, 531, 734, 143, 554, 729, 521, 117],\n",
       " [301, 298, 108, 247, 206, 352, 461, 91, 53, 407],\n",
       " [1058, 576, 77, 338, 275, 329, 83, 96, 514, 276],\n",
       " [50, 10, 339, 65, 145, 151, 59, 137, 117, 16],\n",
       " [170, 132, 165, 983, 161, 562, 536, 838, 162, 152],\n",
       " [213, 280, 352, 974, 239, 63, 207, 19, 57, 201],\n",
       " [552, 546, 338, 763, 493, 329, 265, 40, 512, 361],\n",
       " [122, 140, 179, 534, 175, 460, 195, 158, 564, 161],\n",
       " [866, 561, 140, 176, 195, 193, 158, 161, 154, 196],\n",
       " [463, 117, 33, 201, 96, 52, 59, 512, 276, 16],\n",
       " [195, 167, 536, 192, 162, 152, 838, 562, 161, 132],\n",
       " [206, 137, 96, 134, 158, 67, 161, 131, 140, 33],\n",
       " [28, 152, 149, 16, 366, 112, 276, 200, 219, 576],\n",
       " [576, 267, 201, 5, 2, 137, 140, 96, 154, 33],\n",
       " [137, 420, 336, 70, 67, 105, 290, 357, 205, 108],\n",
       " [128, 5, 562, 267, 239, 33, 415, 201, 544, 464],\n",
       " [53, 471, 658, 299, 352, 96, 286, 272, 554, 239],\n",
       " [154, 583, 4, 458, 544, 97, 5, 338, 383, 329],\n",
       " [96, 484, 5, 464, 239, 354, 250, 472, 33, 137],\n",
       " [95, 207, 91, 67, 145, 116, 3, 82, 8, 33],\n",
       " [239, 5, 662, 145, 158, 201, 142, 415, 383, 128],\n",
       " [18, 513, 352, 281, 91, 33, 89, 107, 191, 212],\n",
       " [33, 137, 357, 136, 152, 354, 72, 67, 144, 7],\n",
       " [354, 513, 92, 191, 33, 154, 91, 99, 77, 329],\n",
       " [33, 103, 578, 515, 281, 350, 513, 101, 472, 357],\n",
       " [61, 227, 348, 87, 675, 72, 46, 290, 149, 749],\n",
       " [193, 128, 67, 33, 161, 534, 195, 158, 280, 154],\n",
       " [77, 265, 206, 33, 338, 205, 91, 8, 96, 329],\n",
       " [134, 192, 161, 175, 277, 196, 158, 195, 154, 140],\n",
       " [513, 243, 112, 576, 338, 215, 96, 154, 383, 329],\n",
       " [132, 167, 192, 122, 195, 562, 161, 158, 534, 838],\n",
       " [200, 754, 761, 605, 562, 354, 317, 271, 152, 95],\n",
       " [153, 113, 145, 144, 136, 16, 10, 152, 151, 117],\n",
       " [816, 77, 203, 89, 464, 212, 201, 91, 96, 92],\n",
       " [137, 95, 290, 49, 70, 251, 551, 110, 346, 205],\n",
       " [196, 170, 195, 790, 140, 132, 983, 161, 158, 779],\n",
       " [179, 779, 132, 186, 152, 67, 193, 170, 194, 162],\n",
       " [213, 44, 96, 513, 605, 337, 272, 247, 205, 329],\n",
       " [105, 251, 316, 753, 298, 110, 372, 374, 594, 472],\n",
       " [69, 103, 463, 275, 110, 336, 42, 512, 276, 247],\n",
       " [339, 243, 16, 1137, 92, 96, 1016, 513, 219, 201],\n",
       " [269, 276, 118, 514, 357, 448, 576, 209, 275, 338],\n",
       " [482, 275, 304, 484, 354, 152, 379, 346, 205, 250],\n",
       " [175, 132, 170, 195, 561, 790, 161, 196, 779, 983],\n",
       " [59, 110, 191, 96, 513, 605, 506, 18, 512, 48],\n",
       " [195, 196, 162, 152, 134, 140, 161, 562, 154, 132],\n",
       " [580, 383, 158, 214, 19, 25, 239, 415, 201, 96],\n",
       " [348, 284, 429, 488, 33, 291, 144, 594, 273, 136],\n",
       " [269, 290, 206, 324, 483, 205, 8, 329, 491, 97],\n",
       " [905, 195, 123, 157, 64, 142, 193, 5, 154, 576],\n",
       " [122, 152, 194, 556, 128, 154, 175, 534, 161, 158],\n",
       " [70, 274, 592, 89, 483, 205, 491, 33, 472, 290],\n",
       " [336, 108, 346, 274, 70, 105, 206, 281, 49, 205],\n",
       " [35, 194, 576, 118, 95, 269, 276, 162, 154, 551],\n",
       " [161, 471, 205, 45, 93, 158, 37, 465, 393, 336],\n",
       " [195, 177, 536, 357, 152, 140, 162, 324, 67, 161],\n",
       " [16, 512, 299, 203, 96, 272, 448, 338, 53, 513],\n",
       " [749, 315, 317, 152, 38, 273, 136, 95, 284, 151],\n",
       " [96, 336, 33, 122, 148, 415, 45, 280, 531, 158],\n",
       " [89, 101, 484, 158, 420, 281, 33, 205, 357, 67],\n",
       " [274, 67, 107, 152, 33, 141, 192, 110, 836, 97],\n",
       " [141, 564, 790, 158, 152, 177, 195, 562, 983, 161],\n",
       " [604, 51, 117, 329, 48, 96, 59, 512, 243, 16],\n",
       " [128, 716, 33, 137, 336, 5, 96, 464, 239, 201],\n",
       " [310, 309, 594, 311, 472, 299, 101, 92, 103, 205],\n",
       " [205, 108, 93, 95, 207, 110, 578, 49, 290, 97],\n",
       " [342, 1015, 253, 247, 205, 276, 110, 53, 48, 512],\n",
       " [193, 184, 154, 1085, 854, 693, 462, 196, 789, 983],\n",
       " [50, 157, 137, 744, 306, 834, 28, 831, 123, 709],\n",
       " [3, 329, 5, 354, 15, 97, 95, 33, 269, 67],\n",
       " [203, 239, 162, 92, 9, 575, 65, 144, 136, 137],\n",
       " [200, 96, 37, 59, 618, 338, 144, 28, 117, 354],\n",
       " [393, 33, 605, 205, 8, 206, 336, 96, 49, 45],\n",
       " [150, 117, 338, 415, 383, 96, 148, 33, 8, 201],\n",
       " [201, 239, 464, 895, 414, 729, 383, 554, 203, 544],\n",
       " [157, 1029, 5, 66, 9, 609, 61, 151, 117, 383],\n",
       " [290, 578, 464, 354, 207, 91, 191, 89, 33, 472],\n",
       " [132, 983, 177, 838, 508, 564, 175, 158, 562, 161],\n",
       " [92, 49, 33, 338, 77, 102, 269, 142, 329, 8],\n",
       " [534, 154, 141, 561, 132, 562, 195, 838, 158, 161],\n",
       " [308, 511, 132, 86, 149, 144, 273, 151, 152, 136],\n",
       " [1192, 790, 177, 192, 161, 1088, 983, 194, 152, 132],\n",
       " [134, 195, 536, 177, 983, 158, 838, 132, 562, 161],\n",
       " [117, 239, 143, 415, 201, 716, 734, 10, 16, 383],\n",
       " [348, 87, 715, 749, 377, 675, 475, 315, 38, 273],\n",
       " [33, 716, 534, 96, 239, 464, 128, 203, 201, 161],\n",
       " [140, 158, 578, 464, 136, 137, 152, 67, 144, 33],\n",
       " [117, 61, 227, 275, 72, 273, 749, 488, 338, 329],\n",
       " [561, 508, 122, 562, 154, 161, 534, 175, 158, 195],\n",
       " [41, 205, 8, 394, 551, 329, 95, 93, 77, 154],\n",
       " [460, 131, 564, 195, 462, 161, 154, 196, 534, 158],\n",
       " [117, 79, 141, 158, 140, 162, 562, 866, 464, 179],\n",
       " [583, 540, 329, 112, 351, 261, 5, 369, 570, 154],\n",
       " [905, 351, 338, 275, 116, 4, 458, 266, 97, 154],\n",
       " [116, 583, 4, 5, 338, 261, 458, 540, 329, 154],\n",
       " [556, 161, 196, 33, 172, 134, 131, 132, 152, 140],\n",
       " [299, 338, 33, 484, 329, 346, 472, 420, 486, 205],\n",
       " [331, 15, 512, 513, 107, 102, 463, 205, 92, 276],\n",
       " [351, 329, 4, 179, 540, 142, 338, 97, 324, 154],\n",
       " [96, 816, 276, 95, 52, 191, 111, 605, 89, 51],\n",
       " [67, 346, 150, 8, 116, 148, 142, 117, 280, 531],\n",
       " [624, 35, 261, 552, 351, 275, 4, 540, 338, 329],\n",
       " [93, 33, 338, 99, 275, 101, 205, 329, 118, 96],\n",
       " [134, 152, 154, 158, 140, 132, 534, 562, 195, 161],\n",
       " [116, 583, 4, 288, 331, 338, 259, 119, 781, 512],\n",
       " [275, 95, 314, 107, 82, 207, 329, 290, 281, 357],\n",
       " [67, 89, 578, 472, 488, 513, 207, 33, 357, 354],\n",
       " [203, 735, 484, 136, 89, 472, 247, 407, 96, 6],\n",
       " [96, 735, 5, 9, 324, 205, 346, 33, 357, 67],\n",
       " [132, 273, 136, 275, 484, 483, 491, 162, 152, 753],\n",
       " [154, 561, 152, 508, 195, 562, 158, 534, 175, 161],\n",
       " [357, 709, 749, 373, 137, 49, 290, 275, 483, 97],\n",
       " [415, 156, 544, 464, 531, 142, 201, 716, 239, 383],\n",
       " [346, 347, 252, 205, 95, 267, 15, 551, 5, 345],\n",
       " [16, 561, 4, 122, 562, 531, 158, 544, 179, 154],\n",
       " [205, 48, 290, 485, 512, 309, 42, 265, 276, 40],\n",
       " [136, 464, 132, 562, 151, 7, 117, 144, 161, 152],\n",
       " [92, 140, 329, 137, 735, 33, 205, 354, 338, 67],\n",
       " [162, 131, 592, 134, 96, 140, 161, 158, 536, 132],\n",
       " [1075, 107, 756, 333, 207, 825, 158, 239, 332, 191],\n",
       " [465, 213, 580, 33, 41, 329, 471, 336, 96, 352],\n",
       " [611, 191, 128, 33, 77, 91, 96, 464, 239, 201],\n",
       " [117, 413, 544, 977, 201, 148, 415, 521, 239, 383],\n",
       " [336, 269, 91, 193, 67, 196, 140, 290, 194, 33],\n",
       " [91, 69, 110, 59, 357, 472, 329, 735, 512, 205],\n",
       " [96, 44, 361, 8, 77, 338, 116, 331, 342, 329],\n",
       " [167, 162, 564, 194, 536, 838, 562, 152, 132, 161],\n",
       " [141, 838, 149, 493, 143, 575, 117, 464, 48, 472],\n",
       " [136, 464, 16, 10, 383, 152, 145, 201, 59, 33],\n",
       " [5, 531, 716, 201, 16, 203, 142, 544, 239, 383],\n",
       " [684, 564, 141, 561, 177, 195, 562, 158, 838, 161],\n",
       " [1072, 95, 119, 58, 526, 478, 605, 528, 540, 51],\n",
       " [16, 116, 209, 324, 338, 118, 342, 95, 354, 207],\n",
       " [329, 276, 49, 205, 275, 816, 118, 48, 110, 512],\n",
       " [152, 508, 561, 838, 162, 158, 983, 562, 132, 161],\n",
       " [273, 475, 484, 170, 205, 108, 33, 95, 346, 99],\n",
       " [561, 141, 838, 132, 983, 140, 562, 195, 158, 161],\n",
       " [605, 89, 336, 107, 77, 331, 394, 91, 191, 393],\n",
       " [7, 137, 101, 464, 354, 324, 77, 336, 472, 448],\n",
       " [141, 65, 161, 117, 179, 514, 562, 162, 132, 152],\n",
       " [158, 187, 186, 152, 983, 193, 140, 154, 161, 194],\n",
       " [137, 140, 534, 92, 33, 161, 152, 67, 49, 205],\n",
       " [92, 618, 312, 361, 350, 79, 122, 175, 339, 608],\n",
       " [336, 5, 203, 95, 580, 503, 816, 96, 77, 49],\n",
       " [536, 167, 611, 152, 187, 838, 141, 562, 161, 162],\n",
       " [89, 354, 299, 205, 91, 140, 137, 92, 329, 33],\n",
       " [479, 137, 65, 72, 162, 132, 151, 136, 152, 170],\n",
       " [49, 97, 162, 118, 420, 154, 275, 357, 338, 329],\n",
       " [556, 983, 544, 4, 564, 152, 576, 570, 5, 154],\n",
       " [140, 122, 177, 167, 562, 152, 179, 117, 158, 161],\n",
       " [7, 801, 167, 140, 467, 564, 277, 158, 161, 163],\n",
       " [169, 142, 154, 206, 93, 35, 134, 578, 194, 8],\n",
       " [483, 316, 448, 101, 298, 735, 373, 419, 103, 472],\n",
       " [154, 161, 179, 132, 779, 170, 983, 152, 175, 162],\n",
       " [167, 177, 179, 141, 162, 838, 195, 562, 161, 132],\n",
       " [91, 102, 191, 201, 472, 448, 205, 336, 212, 107],\n",
       " [170, 154, 576, 158, 479, 5, 137, 152, 67, 33],\n",
       " [132, 564, 162, 177, 152, 194, 175, 158, 983, 161],\n",
       " [544, 161, 137, 531, 154, 576, 128, 514, 158, 16],\n",
       " [534, 460, 175, 162, 170, 346, 158, 33, 161, 67],\n",
       " [10, 239, 413, 531, 544, 729, 339, 117, 521, 16],\n",
       " [583, 471, 212, 548, 336, 107, 544, 201, 5, 239],\n",
       " [18, 748, 48, 583, 512, 97, 16, 448, 154, 107],\n",
       " [82, 207, 89, 46, 259, 342, 28, 309, 107, 329],\n",
       " [16, 977, 531, 514, 158, 544, 145, 117, 7, 10],\n",
       " [128, 157, 50, 5, 152, 576, 142, 145, 137, 154],\n",
       " [562, 536, 187, 172, 165, 192, 152, 134, 170, 132],\n",
       " [562, 144, 117, 201, 187, 161, 128, 158, 140, 154],\n",
       " [152, 131, 140, 158, 161, 154, 194, 134, 162, 196],\n",
       " [514, 278, 162, 194, 239, 67, 157, 131, 137, 142],\n",
       " [158, 152, 132, 177, 564, 562, 195, 983, 175, 161],\n",
       " [142, 16, 59, 383, 137, 5, 201, 464, 544, 239],\n",
       " [134, 698, 194, 172, 165, 140, 170, 779, 152, 132],\n",
       " [553, 136, 61, 135, 5, 578, 72, 137, 576, 151],\n",
       " [838, 152, 48, 378, 141, 352, 491, 307, 110, 512],\n",
       " [693, 158, 196, 782, 175, 790, 789, 854, 462, 983],\n",
       " [137, 594, 338, 251, 291, 205, 312, 105, 33, 486],\n",
       " [544, 95, 107, 91, 276, 201, 41, 96, 8, 77],\n",
       " [420, 7, 336, 161, 493, 103, 101, 205, 472, 91],\n",
       " [140, 531, 33, 16, 93, 150, 117, 96, 144, 464],\n",
       " [354, 111, 8, 107, 95, 33, 28, 98, 89, 97],\n",
       " [569, 5, 329, 531, 107, 77, 544, 464, 336, 276],\n",
       " [16, 117, 15, 575, 152, 7, 137, 72, 145, 354],\n",
       " [10, 96, 154, 16, 140, 531, 464, 117, 33, 158],\n",
       " [9, 72, 61, 479, 5, 145, 144, 136, 152, 151],\n",
       " [187, 534, 536, 154, 140, 128, 152, 161, 132, 158]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_recommends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('case1segment_recommendations.pkl', 'wb') as f:\n",
    "    pickle.dump(top_k_recommends, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[145, 151, 149, 140, 117, 161, 136, 144, 132, 152],\n",
       " [181, 179, 170, 536, 187, 161, 194, 162, 562, 132],\n",
       " [134, 141, 838, 162, 152, 140, 161, 562, 611, 132],\n",
       " [18, 352, 91, 70, 286, 512, 369, 89, 53, 276],\n",
       " [357, 482, 67, 271, 299, 33, 272, 137, 354, 484],\n",
       " [247, 116, 343, 70, 95, 91, 342, 107, 324, 207],\n",
       " [580, 8, 15, 464, 195, 134, 45, 212, 154, 465],\n",
       " [33, 110, 140, 336, 420, 329, 161, 144, 137, 152],\n",
       " [97, 735, 490, 205, 281, 486, 338, 275, 491, 329],\n",
       " [18, 463, 152, 15, 52, 314, 46, 331, 276, 92],\n",
       " [275, 91, 354, 205, 338, 96, 472, 513, 357, 329],\n",
       " [107, 7, 534, 548, 161, 110, 992, 394, 152, 583],\n",
       " [33, 137, 89, 576, 205, 338, 152, 203, 354, 92],\n",
       " [339, 383, 729, 132, 203, 139, 117, 128, 66, 158],\n",
       " [506, 145, 280, 8, 605, 49, 33, 271, 205, 67],\n",
       " [514, 576, 16, 154, 144, 137, 136, 117, 151, 152],\n",
       " [348, 357, 107, 309, 735, 273, 500, 99, 87, 46],\n",
       " [172, 33, 534, 161, 192, 154, 158, 137, 140, 152],\n",
       " [407, 554, 158, 53, 33, 288, 128, 96, 203, 201],\n",
       " [584, 408, 196, 369, 5, 616, 347, 564, 570, 154],\n",
       " [362, 95, 336, 578, 393, 580, 115, 471, 201, 41],\n",
       " [92, 16, 342, 96, 137, 583, 112, 253, 77, 329],\n",
       " [3, 8, 33, 329, 346, 95, 28, 338, 205, 269],\n",
       " [290, 101, 33, 191, 205, 357, 89, 92, 472, 91],\n",
       " [142, 33, 544, 383, 137, 5, 128, 201, 239, 464],\n",
       " [727, 122, 239, 157, 724, 280, 179, 154, 383, 521],\n",
       " [275, 5, 18, 351, 4, 259, 458, 261, 329, 154],\n",
       " [105, 275, 95, 290, 97, 162, 324, 357, 170, 483],\n",
       " [838, 122, 128, 154, 329, 33, 562, 195, 96, 158],\n",
       " [186, 193, 290, 484, 275, 357, 491, 162, 483, 170],\n",
       " [196, 158, 464, 195, 415, 239, 161, 544, 534, 128],\n",
       " [562, 154, 196, 177, 983, 152, 534, 564, 158, 161],\n",
       " [779, 838, 534, 562, 175, 158, 162, 983, 152, 161],\n",
       " [67, 564, 460, 33, 154, 161, 196, 140, 158, 195],\n",
       " [838, 564, 508, 175, 779, 790, 179, 161, 685, 983],\n",
       " [716, 33, 96, 5, 383, 415, 464, 239, 544, 201],\n",
       " [52, 512, 463, 583, 158, 407, 203, 544, 554, 276],\n",
       " [195, 158, 141, 192, 175, 152, 838, 562, 132, 161],\n",
       " [168, 170, 185, 152, 141, 162, 161, 562, 838, 132],\n",
       " [91, 338, 551, 357, 102, 448, 70, 101, 137, 205],\n",
       " [89, 354, 96, 18, 10, 276, 16, 331, 37, 117],\n",
       " [143, 200, 59, 42, 140, 144, 52, 117, 149, 16],\n",
       " [407, 96, 92, 342, 77, 201, 33, 247, 91, 53],\n",
       " [531, 521, 201, 16, 544, 415, 10, 239, 716, 383],\n",
       " [192, 140, 131, 162, 158, 170, 194, 132, 152, 161],\n",
       " [508, 164, 779, 840, 132, 162, 152, 161, 983, 194],\n",
       " [281, 324, 105, 108, 101, 357, 314, 275, 103, 420],\n",
       " [50, 192, 42, 2, 219, 576, 709, 28, 52, 276],\n",
       " [123, 65, 193, 154, 131, 142, 5, 137, 50, 576],\n",
       " [207, 275, 472, 97, 338, 336, 324, 206, 205, 329],\n",
       " [16, 10, 117, 152, 5, 144, 137, 33, 203, 136],\n",
       " [67, 339, 716, 239, 338, 137, 148, 329, 544, 5],\n",
       " [544, 179, 148, 977, 157, 239, 142, 383, 201, 415],\n",
       " [16, 716, 5, 464, 203, 544, 96, 33, 239, 201],\n",
       " [1, 96, 239, 201, 716, 478, 191, 471, 463, 512],\n",
       " [716, 5, 415, 128, 33, 383, 544, 203, 201, 239],\n",
       " [77, 464, 15, 96, 132, 92, 205, 551, 338, 137],\n",
       " [110, 488, 259, 89, 346, 309, 338, 331, 116, 205],\n",
       " [105, 352, 298, 207, 91, 212, 816, 77, 290, 472],\n",
       " [194, 162, 192, 158, 140, 838, 562, 152, 132, 161],\n",
       " [5, 92, 107, 201, 91, 33, 354, 383, 45, 15],\n",
       " [144, 136, 162, 239, 131, 65, 137, 140, 132, 152],\n",
       " [89, 92, 206, 353, 354, 463, 91, 513, 464, 33],\n",
       " [10, 276, 331, 59, 117, 205, 201, 96, 512, 48],\n",
       " [531, 117, 544, 464, 201, 161, 33, 128, 96, 158],\n",
       " [895, 414, 96, 534, 196, 383, 460, 117, 16, 154],\n",
       " [59, 137, 201, 239, 5, 544, 383, 10, 117, 16],\n",
       " [329, 179, 137, 33, 142, 576, 514, 131, 67, 338],\n",
       " [463, 464, 342, 96, 352, 52, 301, 247, 407, 53],\n",
       " [269, 346, 33, 483, 110, 152, 137, 205, 162, 67],\n",
       " [205, 150, 409, 531, 331, 67, 275, 117, 329, 338],\n",
       " [132, 33, 131, 91, 111, 576, 96, 562, 95, 152],\n",
       " [162, 141, 67, 132, 195, 158, 152, 140, 562, 161],\n",
       " [16, 531, 10, 464, 117, 544, 716, 383, 203, 239],\n",
       " [95, 104, 183, 83, 154, 193, 273, 28, 50, 576],\n",
       " [338, 336, 506, 271, 102, 329, 91, 137, 8, 205],\n",
       " [158, 142, 576, 117, 531, 137, 5, 16, 544, 154],\n",
       " [165, 479, 162, 72, 61, 138, 170, 152, 136, 151],\n",
       " [484, 357, 67, 170, 132, 161, 354, 140, 33, 152],\n",
       " [142, 383, 137, 239, 148, 575, 514, 544, 117, 16],\n",
       " [201, 140, 463, 103, 42, 96, 117, 464, 33, 407],\n",
       " [339, 40, 59, 716, 158, 148, 729, 117, 512, 531],\n",
       " [152, 179, 415, 16, 143, 409, 149, 145, 575, 117],\n",
       " [735, 212, 281, 93, 290, 227, 506, 239, 99, 18],\n",
       " [195, 167, 158, 185, 564, 838, 175, 562, 161, 132],\n",
       " [819, 214, 722, 541, 48, 329, 383, 40, 512, 205],\n",
       " [158, 152, 140, 195, 838, 162, 170, 161, 562, 132],\n",
       " [651, 618, 132, 149, 117, 144, 16, 96, 151, 59],\n",
       " [753, 34, 483, 7, 136, 151, 110, 144, 488, 152],\n",
       " [102, 464, 513, 486, 108, 91, 337, 15, 98, 205],\n",
       " [874, 276, 504, 119, 44, 716, 336, 67, 329, 269],\n",
       " [201, 5, 137, 152, 339, 144, 203, 117, 10, 16],\n",
       " [1, 107, 816, 708, 329, 92, 583, 51, 512, 275],\n",
       " [463, 265, 342, 92, 513, 205, 91, 506, 107, 247],\n",
       " [92, 191, 206, 205, 207, 91, 513, 357, 89, 107],\n",
       " [93, 95, 92, 91, 33, 15, 49, 89, 205, 206],\n",
       " [314, 709, 210, 281, 512, 491, 275, 309, 101, 110],\n",
       " [534, 158, 145, 140, 142, 694, 544, 33, 5, 154],\n",
       " [170, 562, 141, 134, 158, 140, 161, 279, 779, 132],\n",
       " [857, 734, 544, 339, 569, 203, 415, 383, 239, 201],\n",
       " [458, 394, 281, 77, 583, 19, 211, 154, 463, 212],\n",
       " [161, 5, 201, 239, 128, 158, 96, 205, 33, 137],\n",
       " [175, 195, 158, 122, 167, 564, 154, 152, 534, 161],\n",
       " [564, 562, 195, 154, 140, 175, 132, 152, 158, 161],\n",
       " [342, 7, 207, 354, 144, 191, 110, 91, 472, 107],\n",
       " [163, 162, 562, 175, 564, 534, 158, 132, 161, 152],\n",
       " [201, 144, 203, 239, 117, 140, 128, 10, 544, 16],\n",
       " [531, 212, 336, 394, 96, 41, 276, 214, 362, 211],\n",
       " [362, 8, 478, 214, 605, 997, 406, 1017, 19, 211],\n",
       " [158, 167, 162, 187, 168, 152, 132, 161, 838, 562],\n",
       " [170, 152, 838, 158, 175, 141, 177, 132, 161, 562],\n",
       " [983, 564, 462, 158, 195, 175, 152, 193, 196, 154],\n",
       " [345, 329, 357, 5, 420, 484, 49, 483, 290, 206],\n",
       " [531, 205, 357, 338, 142, 269, 100, 97, 324, 67],\n",
       " [170, 140, 133, 141, 536, 183, 195, 152, 562, 161],\n",
       " [64, 8, 215, 5, 239, 142, 201, 267, 33, 383],\n",
       " [464, 205, 5, 95, 206, 15, 347, 77, 338, 329],\n",
       " [154, 201, 96, 33, 137, 464, 562, 140, 5, 239],\n",
       " [158, 137, 67, 716, 128, 33, 544, 5, 464, 201],\n",
       " [575, 72, 578, 151, 65, 117, 162, 136, 144, 137],\n",
       " [754, 304, 337, 334, 354, 250, 513, 605, 205, 331],\n",
       " [103, 275, 152, 137, 33, 354, 338, 324, 304, 67],\n",
       " [137, 464, 91, 329, 239, 96, 77, 5, 201, 33],\n",
       " [91, 200, 107, 53, 276, 52, 407, 512, 96, 128],\n",
       " [177, 131, 536, 134, 838, 185, 195, 562, 161, 132],\n",
       " [132, 158, 96, 562, 195, 161, 154, 134, 464, 140],\n",
       " [338, 16, 247, 48, 276, 562, 331, 329, 342, 117],\n",
       " [564, 158, 562, 838, 167, 175, 132, 152, 162, 161],\n",
       " [195, 1083, 866, 659, 157, 184, 8, 214, 415, 123],\n",
       " [731, 172, 157, 137, 152, 1000, 162, 138, 132, 479],\n",
       " [271, 315, 309, 49, 346, 357, 273, 162, 275, 95],\n",
       " [473, 460, 195, 140, 158, 564, 132, 562, 152, 161],\n",
       " [132, 195, 179, 534, 152, 838, 122, 562, 158, 161],\n",
       " [96, 67, 102, 140, 338, 142, 8, 33, 329, 154],\n",
       " [149, 33, 117, 137, 5, 67, 136, 151, 144, 152],\n",
       " [544, 464, 534, 140, 128, 203, 161, 201, 239, 158],\n",
       " [464, 33, 354, 137, 551, 329, 67, 276, 338, 205],\n",
       " [144, 578, 151, 136, 117, 161, 145, 132, 162, 152],\n",
       " [983, 564, 140, 175, 838, 562, 152, 158, 132, 161],\n",
       " [203, 161, 339, 148, 534, 122, 201, 179, 79, 158],\n",
       " [161, 536, 192, 562, 170, 134, 162, 132, 152, 140],\n",
       " [177, 508, 154, 122, 152, 140, 195, 562, 158, 161],\n",
       " [161, 67, 357, 144, 72, 338, 329, 354, 152, 162],\n",
       " [1052, 828, 383, 413, 561, 214, 161, 195, 239, 201],\n",
       " [140, 562, 175, 131, 177, 158, 152, 132, 534, 161],\n",
       " [162, 562, 277, 460, 132, 152, 564, 175, 158, 161],\n",
       " [342, 207, 92, 49, 276, 89, 111, 448, 331, 205],\n",
       " [172, 779, 134, 195, 131, 161, 170, 152, 162, 132],\n",
       " [161, 162, 576, 5, 132, 140, 194, 131, 137, 152],\n",
       " [48, 482, 472, 338, 89, 101, 207, 354, 110, 342],\n",
       " [604, 16, 763, 361, 40, 265, 276, 729, 48, 512],\n",
       " [206, 116, 102, 8, 97, 45, 98, 205, 67, 33],\n",
       " [338, 483, 98, 290, 93, 324, 357, 205, 33, 67],\n",
       " [154, 162, 179, 195, 561, 983, 152, 175, 158, 161],\n",
       " [483, 336, 357, 67, 269, 578, 7, 271, 754, 354],\n",
       " [7, 59, 544, 128, 201, 239, 144, 10, 16, 117],\n",
       " [338, 216, 575, 92, 46, 471, 142, 15, 5, 383],\n",
       " [383, 128, 148, 413, 717, 831, 201, 96, 415, 158],\n",
       " [95, 205, 576, 145, 5, 142, 712, 67, 33, 137],\n",
       " [205, 210, 407, 95, 70, 89, 448, 91, 90, 472],\n",
       " [383, 128, 267, 125, 239, 33, 158, 154, 96, 201],\n",
       " [536, 172, 140, 838, 562, 187, 161, 192, 132, 152],\n",
       " [161, 718, 278, 544, 977, 128, 415, 239, 201, 383],\n",
       " [158, 161, 140, 137, 152, 77, 464, 576, 33, 96],\n",
       " [154, 561, 177, 152, 179, 562, 195, 140, 158, 161],\n",
       " [354, 513, 306, 205, 271, 209, 286, 101, 210, 448],\n",
       " [5, 96, 415, 128, 575, 464, 117, 383, 201, 544],\n",
       " [152, 298, 735, 420, 486, 291, 307, 105, 484, 132],\n",
       " [194, 179, 140, 196, 564, 561, 158, 195, 154, 161],\n",
       " [167, 170, 838, 132, 164, 162, 152, 983, 779, 161],\n",
       " [346, 753, 205, 354, 378, 488, 304, 250, 273, 324],\n",
       " [161, 685, 140, 152, 790, 779, 132, 170, 983, 158],\n",
       " [729, 10, 558, 59, 144, 716, 96, 512, 16, 48],\n",
       " [331, 67, 8, 97, 513, 329, 33, 205, 354, 338],\n",
       " [161, 514, 131, 172, 194, 152, 137, 154, 5, 576],\n",
       " [102, 97, 96, 33, 154, 8, 195, 131, 338, 329],\n",
       " [92, 415, 269, 117, 45, 42, 354, 33, 329, 8],\n",
       " [983, 561, 611, 175, 694, 195, 564, 132, 562, 161],\n",
       " [540, 761, 117, 488, 583, 107, 448, 200, 276, 16],\n",
       " [804, 983, 849, 56, 1044, 531, 521, 916, 1154, 801],\n",
       " [914, 472, 69, 70, 105, 33, 110, 205, 10, 96],\n",
       " [33, 211, 205, 214, 362, 19, 329, 201, 478, 96],\n",
       " [103, 435, 59, 493, 275, 151, 136, 221, 7, 749],\n",
       " [136, 472, 33, 314, 354, 281, 67, 357, 152, 7],\n",
       " [162, 175, 983, 79, 158, 134, 195, 140, 131, 179],\n",
       " [200, 329, 338, 111, 281, 107, 101, 92, 551, 205],\n",
       " [211, 96, 7, 324, 150, 212, 281, 329, 205, 67],\n",
       " [348, 103, 372, 291, 87, 290, 299, 136, 70, 484],\n",
       " [140, 415, 464, 718, 239, 33, 201, 128, 161, 158],\n",
       " [185, 183, 168, 141, 187, 161, 162, 838, 562, 132],\n",
       " [383, 16, 239, 154, 142, 145, 5, 137, 67, 33],\n",
       " [338, 276, 195, 329, 512, 608, 534, 161, 158, 96],\n",
       " [393, 290, 110, 735, 105, 207, 336, 274, 608, 97],\n",
       " [70, 116, 120, 96, 128, 104, 265, 158, 234, 460],\n",
       " [16, 72, 203, 59, 144, 9, 61, 152, 151, 136],\n",
       " [212, 8, 562, 195, 41, 611, 134, 362, 211, 19],\n",
       " [96, 99, 239, 33, 77, 137, 16, 154, 329, 201],\n",
       " [183, 611, 128, 138, 172, 151, 161, 562, 132, 152],\n",
       " [52, 191, 33, 463, 286, 512, 239, 212, 89, 18],\n",
       " [35, 269, 331, 354, 338, 15, 540, 154, 212, 329],\n",
       " [101, 336, 329, 92, 102, 191, 338, 137, 205, 357],\n",
       " [142, 5, 464, 195, 140, 161, 531, 117, 158, 154],\n",
       " [179, 131, 165, 161, 140, 162, 562, 170, 132, 152],\n",
       " [175, 132, 840, 779, 194, 158, 161, 983, 170, 162],\n",
       " [536, 164, 846, 838, 279, 779, 168, 983, 162, 185],\n",
       " [7, 544, 48, 150, 201, 101, 59, 357, 118, 281],\n",
       " [493, 350, 265, 450, 342, 200, 605, 110, 512, 48],\n",
       " [215, 154, 514, 96, 145, 383, 137, 33, 142, 576],\n",
       " [451, 33, 118, 210, 512, 154, 205, 281, 96, 212],\n",
       " [464, 53, 467, 295, 298, 140, 69, 407, 105, 247],\n",
       " [314, 194, 354, 754, 15, 92, 562, 162, 152, 132],\n",
       " [96, 275, 324, 205, 544, 201, 338, 16, 33, 276],\n",
       " [131, 28, 329, 817, 269, 77, 205, 137, 551, 95],\n",
       " [158, 512, 276, 49, 206, 605, 96, 93, 329, 205],\n",
       " [448, 28, 2, 45, 142, 96, 514, 15, 97, 329],\n",
       " [336, 415, 352, 465, 471, 383, 544, 239, 96, 201],\n",
       " [97, 551, 324, 137, 338, 67, 92, 49, 205, 329],\n",
       " [97, 317, 9, 151, 576, 95, 136, 67, 33, 152],\n",
       " [170, 158, 162, 140, 141, 779, 161, 152, 562, 132],\n",
       " [96, 207, 45, 269, 33, 97, 336, 329, 102, 8],\n",
       " [460, 338, 191, 471, 96, 101, 276, 48, 512, 478],\n",
       " [383, 464, 801, 195, 239, 158, 729, 140, 117, 201],\n",
       " [871, 326, 469, 409, 497, 450, 117, 179, 761, 171],\n",
       " [108, 102, 472, 67, 33, 101, 98, 357, 91, 205],\n",
       " [269, 33, 712, 77, 576, 464, 95, 5, 137, 152],\n",
       " [16, 831, 152, 117, 576, 157, 162, 136, 151, 145],\n",
       " [513, 6, 161, 144, 33, 158, 562, 92, 140, 132],\n",
       " [536, 168, 141, 170, 562, 838, 161, 162, 152, 132],\n",
       " [239, 352, 201, 93, 464, 91, 33, 96, 191, 205],\n",
       " [152, 10, 562, 464, 128, 117, 140, 575, 158, 161],\n",
       " [100, 103, 290, 269, 275, 170, 271, 749, 329, 273],\n",
       " [513, 271, 342, 354, 272, 42, 346, 33, 67, 205],\n",
       " [483, 305, 59, 103, 205, 423, 338, 914, 735, 259],\n",
       " [33, 91, 99, 271, 205, 140, 354, 92, 420, 357],\n",
       " [314, 339, 148, 357, 716, 203, 201, 239, 150, 7],\n",
       " [49, 483, 754, 205, 250, 95, 70, 484, 290, 472],\n",
       " [152, 163, 167, 575, 534, 562, 117, 140, 158, 161],\n",
       " [49, 95, 273, 137, 484, 817, 749, 70, 275, 33],\n",
       " [162, 175, 611, 158, 562, 132, 195, 983, 177, 161],\n",
       " [983, 179, 840, 132, 193, 162, 194, 186, 154, 170],\n",
       " [576, 61, 145, 117, 144, 149, 154, 136, 152, 151],\n",
       " [151, 331, 488, 152, 276, 10, 59, 144, 117, 16],\n",
       " [77, 7, 119, 145, 8, 28, 201, 354, 761, 95],\n",
       " [10, 96, 239, 544, 716, 729, 117, 16, 895, 554],\n",
       " [205, 544, 83, 5, 96, 383, 77, 336, 338, 329],\n",
       " [536, 92, 134, 137, 562, 152, 131, 161, 132, 140],\n",
       " [354, 5, 137, 329, 464, 205, 140, 15, 33, 338],\n",
       " [269, 92, 336, 33, 354, 464, 338, 137, 5, 329],\n",
       " [131, 152, 179, 162, 983, 154, 269, 576, 140, 464],\n",
       " [464, 137, 59, 544, 354, 575, 201, 33, 117, 383],\n",
       " [9, 531, 203, 16, 280, 117, 152, 734, 145, 10],\n",
       " [96, 484, 354, 45, 329, 91, 158, 464, 336, 33],\n",
       " [239, 352, 92, 53, 201, 299, 247, 463, 554, 286],\n",
       " [694, 383, 5, 196, 616, 179, 544, 4, 570, 154],\n",
       " [575, 96, 179, 464, 338, 25, 329, 276, 117, 531],\n",
       " [144, 131, 576, 479, 273, 151, 72, 172, 136, 152],\n",
       " [177, 162, 562, 158, 152, 179, 140, 195, 132, 161],\n",
       " [49, 137, 92, 239, 33, 201, 28, 338, 329, 96],\n",
       " [779, 185, 536, 983, 194, 562, 162, 152, 132, 161],\n",
       " [175, 599, 790, 196, 277, 158, 163, 161, 534, 564],\n",
       " [394, 42, 354, 583, 16, 513, 53, 342, 89, 107],\n",
       " [206, 70, 91, 105, 564, 205, 281, 101, 250, 161],\n",
       " [110, 116, 276, 576, 338, 5, 137, 154, 151, 152],\n",
       " [67, 5, 117, 96, 137, 148, 154, 544, 531, 158],\n",
       " [16, 136, 137, 161, 140, 117, 145, 10, 144, 152],\n",
       " [580, 77, 41, 329, 49, 461, 352, 239, 465, 213],\n",
       " [49, 92, 336, 191, 8, 212, 269, 83, 77, 329],\n",
       " [636, 354, 210, 493, 118, 709, 754, 275, 448, 338],\n",
       " [5, 59, 261, 583, 576, 351, 4, 338, 329, 275],\n",
       " [482, 7, 89, 67, 306, 754, 501, 578, 205, 378],\n",
       " [329, 338, 357, 483, 95, 89, 49, 275, 206, 97],\n",
       " [536, 170, 140, 141, 187, 838, 132, 152, 161, 562],\n",
       " [977, 148, 16, 531, 734, 143, 554, 729, 521, 117],\n",
       " [301, 298, 108, 247, 206, 352, 461, 91, 53, 407],\n",
       " [1058, 576, 77, 338, 275, 329, 83, 96, 514, 276],\n",
       " [50, 10, 339, 65, 145, 151, 59, 137, 117, 16],\n",
       " [170, 132, 165, 983, 161, 562, 536, 838, 162, 152],\n",
       " [213, 280, 352, 974, 239, 63, 207, 19, 57, 201],\n",
       " [552, 546, 338, 763, 493, 329, 265, 40, 512, 361],\n",
       " [122, 140, 179, 534, 175, 460, 195, 158, 564, 161],\n",
       " [866, 561, 140, 176, 195, 193, 158, 161, 154, 196],\n",
       " [463, 117, 33, 201, 96, 52, 59, 512, 276, 16],\n",
       " [195, 167, 536, 192, 162, 152, 838, 562, 161, 132],\n",
       " [206, 137, 96, 134, 158, 67, 161, 131, 140, 33],\n",
       " [28, 152, 149, 16, 366, 112, 276, 200, 219, 576],\n",
       " [576, 267, 201, 5, 2, 137, 140, 96, 154, 33],\n",
       " [137, 420, 336, 70, 67, 105, 290, 357, 205, 108],\n",
       " [128, 5, 562, 267, 239, 33, 415, 201, 544, 464],\n",
       " [53, 471, 658, 299, 352, 96, 286, 272, 554, 239],\n",
       " [154, 583, 4, 458, 544, 97, 5, 338, 383, 329],\n",
       " [96, 484, 5, 464, 239, 354, 250, 472, 33, 137],\n",
       " [95, 207, 91, 67, 145, 116, 3, 82, 8, 33],\n",
       " [239, 5, 662, 145, 158, 201, 142, 415, 383, 128],\n",
       " [18, 513, 352, 281, 91, 33, 89, 107, 191, 212],\n",
       " [33, 137, 357, 136, 152, 354, 72, 67, 144, 7],\n",
       " [354, 513, 92, 191, 33, 154, 91, 99, 77, 329],\n",
       " [33, 103, 578, 515, 281, 350, 513, 101, 472, 357],\n",
       " [61, 227, 348, 87, 675, 72, 46, 290, 149, 749],\n",
       " [193, 128, 67, 33, 161, 534, 195, 158, 280, 154],\n",
       " [77, 265, 206, 33, 338, 205, 91, 8, 96, 329],\n",
       " [134, 192, 161, 175, 277, 196, 158, 195, 154, 140],\n",
       " [513, 243, 112, 576, 338, 215, 96, 154, 383, 329],\n",
       " [132, 167, 192, 122, 195, 562, 161, 158, 534, 838],\n",
       " [200, 754, 761, 605, 562, 354, 317, 271, 152, 95],\n",
       " [153, 113, 145, 144, 136, 16, 10, 152, 151, 117],\n",
       " [816, 77, 203, 89, 464, 212, 201, 91, 96, 92],\n",
       " [137, 95, 290, 49, 70, 251, 551, 110, 346, 205],\n",
       " [196, 170, 195, 790, 140, 132, 983, 161, 158, 779],\n",
       " [179, 779, 132, 186, 152, 67, 193, 170, 194, 162],\n",
       " [213, 44, 96, 513, 605, 337, 272, 247, 205, 329],\n",
       " [105, 251, 316, 753, 298, 110, 372, 374, 594, 472],\n",
       " [69, 103, 463, 275, 110, 336, 42, 512, 276, 247],\n",
       " [339, 243, 16, 1137, 92, 96, 1016, 513, 219, 201],\n",
       " [269, 276, 118, 514, 357, 448, 576, 209, 275, 338],\n",
       " [482, 275, 304, 484, 354, 152, 379, 346, 205, 250],\n",
       " [175, 132, 170, 195, 561, 790, 161, 196, 779, 983],\n",
       " [59, 110, 191, 96, 513, 605, 506, 18, 512, 48],\n",
       " [195, 196, 162, 152, 134, 140, 161, 562, 154, 132],\n",
       " [580, 383, 158, 214, 19, 25, 239, 415, 201, 96],\n",
       " [348, 284, 429, 488, 33, 291, 144, 594, 273, 136],\n",
       " [269, 290, 206, 324, 483, 205, 8, 329, 491, 97],\n",
       " [905, 195, 123, 157, 64, 142, 193, 5, 154, 576],\n",
       " [122, 152, 194, 556, 128, 154, 175, 534, 161, 158],\n",
       " [70, 274, 592, 89, 483, 205, 491, 33, 472, 290],\n",
       " [336, 108, 346, 274, 70, 105, 206, 281, 49, 205],\n",
       " [35, 194, 576, 118, 95, 269, 276, 162, 154, 551],\n",
       " [161, 471, 205, 45, 93, 158, 37, 465, 393, 336],\n",
       " [195, 177, 536, 357, 152, 140, 162, 324, 67, 161],\n",
       " [16, 512, 299, 203, 96, 272, 448, 338, 53, 513],\n",
       " [749, 315, 317, 152, 38, 273, 136, 95, 284, 151],\n",
       " [96, 336, 33, 122, 148, 415, 45, 280, 531, 158],\n",
       " [89, 101, 484, 158, 420, 281, 33, 205, 357, 67],\n",
       " [274, 67, 107, 152, 33, 141, 192, 110, 836, 97],\n",
       " [141, 564, 790, 158, 152, 177, 195, 562, 983, 161],\n",
       " [604, 51, 117, 329, 48, 96, 59, 512, 243, 16],\n",
       " [128, 716, 33, 137, 336, 5, 96, 464, 239, 201],\n",
       " [310, 309, 594, 311, 472, 299, 101, 92, 103, 205],\n",
       " [205, 108, 93, 95, 207, 110, 578, 49, 290, 97],\n",
       " [342, 1015, 253, 247, 205, 276, 110, 53, 48, 512],\n",
       " [193, 184, 154, 1085, 854, 693, 462, 196, 789, 983],\n",
       " [50, 157, 137, 744, 306, 834, 28, 831, 123, 709],\n",
       " [3, 329, 5, 354, 15, 97, 95, 33, 269, 67],\n",
       " [203, 239, 162, 92, 9, 575, 65, 144, 136, 137],\n",
       " [200, 96, 37, 59, 618, 338, 144, 28, 117, 354],\n",
       " [393, 33, 605, 205, 8, 206, 336, 96, 49, 45],\n",
       " [150, 117, 338, 415, 383, 96, 148, 33, 8, 201],\n",
       " [201, 239, 464, 895, 414, 729, 383, 554, 203, 544],\n",
       " [157, 1029, 5, 66, 9, 609, 61, 151, 117, 383],\n",
       " [290, 578, 464, 354, 207, 91, 191, 89, 33, 472],\n",
       " [132, 983, 177, 838, 508, 564, 175, 158, 562, 161],\n",
       " [92, 49, 33, 338, 77, 102, 269, 142, 329, 8],\n",
       " [534, 154, 141, 561, 132, 562, 195, 838, 158, 161],\n",
       " [308, 511, 132, 86, 149, 144, 273, 151, 152, 136],\n",
       " [1192, 790, 177, 192, 161, 1088, 983, 194, 152, 132],\n",
       " [134, 195, 536, 177, 983, 158, 838, 132, 562, 161],\n",
       " [117, 239, 143, 415, 201, 716, 734, 10, 16, 383],\n",
       " [348, 87, 715, 749, 377, 675, 475, 315, 38, 273],\n",
       " [33, 716, 534, 96, 239, 464, 128, 203, 201, 161],\n",
       " [140, 158, 578, 464, 136, 137, 152, 67, 144, 33],\n",
       " [117, 61, 227, 275, 72, 273, 749, 488, 338, 329],\n",
       " [561, 508, 122, 562, 154, 161, 534, 175, 158, 195],\n",
       " [41, 205, 8, 394, 551, 329, 95, 93, 77, 154],\n",
       " [460, 131, 564, 195, 462, 161, 154, 196, 534, 158],\n",
       " [117, 79, 141, 158, 140, 162, 562, 866, 464, 179],\n",
       " [583, 540, 329, 112, 351, 261, 5, 369, 570, 154],\n",
       " [905, 351, 338, 275, 116, 4, 458, 266, 97, 154],\n",
       " [116, 583, 4, 5, 338, 261, 458, 540, 329, 154],\n",
       " [556, 161, 196, 33, 172, 134, 131, 132, 152, 140],\n",
       " [299, 338, 33, 484, 329, 346, 472, 420, 486, 205],\n",
       " [331, 15, 512, 513, 107, 102, 463, 205, 92, 276],\n",
       " [351, 329, 4, 179, 540, 142, 338, 97, 324, 154],\n",
       " [96, 816, 276, 95, 52, 191, 111, 605, 89, 51],\n",
       " [67, 346, 150, 8, 116, 148, 142, 117, 280, 531],\n",
       " [624, 35, 261, 552, 351, 275, 4, 540, 338, 329],\n",
       " [93, 33, 338, 99, 275, 101, 205, 329, 118, 96],\n",
       " [134, 152, 154, 158, 140, 132, 534, 562, 195, 161],\n",
       " [116, 583, 4, 288, 331, 338, 259, 119, 781, 512],\n",
       " [275, 95, 314, 107, 82, 207, 329, 290, 281, 357],\n",
       " [67, 89, 578, 472, 488, 513, 207, 33, 357, 354],\n",
       " [203, 735, 484, 136, 89, 472, 247, 407, 96, 6],\n",
       " [96, 735, 5, 9, 324, 205, 346, 33, 357, 67],\n",
       " [132, 273, 136, 275, 484, 483, 491, 162, 152, 753],\n",
       " [154, 561, 152, 508, 195, 562, 158, 534, 175, 161],\n",
       " [357, 709, 749, 373, 137, 49, 290, 275, 483, 97],\n",
       " [415, 156, 544, 464, 531, 142, 201, 716, 239, 383],\n",
       " [346, 347, 252, 205, 95, 267, 15, 551, 5, 345],\n",
       " [16, 561, 4, 122, 562, 531, 158, 544, 179, 154],\n",
       " [205, 48, 290, 485, 512, 309, 42, 265, 276, 40],\n",
       " [136, 464, 132, 562, 151, 7, 117, 144, 161, 152],\n",
       " [92, 140, 329, 137, 735, 33, 205, 354, 338, 67],\n",
       " [162, 131, 592, 134, 96, 140, 161, 158, 536, 132],\n",
       " [1075, 107, 756, 333, 207, 825, 158, 239, 332, 191],\n",
       " [465, 213, 580, 33, 41, 329, 471, 336, 96, 352],\n",
       " [611, 191, 128, 33, 77, 91, 96, 464, 239, 201],\n",
       " [117, 413, 544, 977, 201, 148, 415, 521, 239, 383],\n",
       " [336, 269, 91, 193, 67, 196, 140, 290, 194, 33],\n",
       " [91, 69, 110, 59, 357, 472, 329, 735, 512, 205],\n",
       " [96, 44, 361, 8, 77, 338, 116, 331, 342, 329],\n",
       " [167, 162, 564, 194, 536, 838, 562, 152, 132, 161],\n",
       " [141, 838, 149, 493, 143, 575, 117, 464, 48, 472],\n",
       " [136, 464, 16, 10, 383, 152, 145, 201, 59, 33],\n",
       " [5, 531, 716, 201, 16, 203, 142, 544, 239, 383],\n",
       " [684, 564, 141, 561, 177, 195, 562, 158, 838, 161],\n",
       " [1072, 95, 119, 58, 526, 478, 605, 528, 540, 51],\n",
       " [16, 116, 209, 324, 338, 118, 342, 95, 354, 207],\n",
       " [329, 276, 49, 205, 275, 816, 118, 48, 110, 512],\n",
       " [152, 508, 561, 838, 162, 158, 983, 562, 132, 161],\n",
       " [273, 475, 484, 170, 205, 108, 33, 95, 346, 99],\n",
       " [561, 141, 838, 132, 983, 140, 562, 195, 158, 161],\n",
       " [605, 89, 336, 107, 77, 331, 394, 91, 191, 393],\n",
       " [7, 137, 101, 464, 354, 324, 77, 336, 472, 448],\n",
       " [141, 65, 161, 117, 179, 514, 562, 162, 132, 152],\n",
       " [158, 187, 186, 152, 983, 193, 140, 154, 161, 194],\n",
       " [137, 140, 534, 92, 33, 161, 152, 67, 49, 205],\n",
       " [92, 618, 312, 361, 350, 79, 122, 175, 339, 608],\n",
       " [336, 5, 203, 95, 580, 503, 816, 96, 77, 49],\n",
       " [536, 167, 611, 152, 187, 838, 141, 562, 161, 162],\n",
       " [89, 354, 299, 205, 91, 140, 137, 92, 329, 33],\n",
       " [479, 137, 65, 72, 162, 132, 151, 136, 152, 170],\n",
       " [49, 97, 162, 118, 420, 154, 275, 357, 338, 329],\n",
       " [556, 983, 544, 4, 564, 152, 576, 570, 5, 154],\n",
       " [140, 122, 177, 167, 562, 152, 179, 117, 158, 161],\n",
       " [7, 801, 167, 140, 467, 564, 277, 158, 161, 163],\n",
       " [169, 142, 154, 206, 93, 35, 134, 578, 194, 8],\n",
       " [483, 316, 448, 101, 298, 735, 373, 419, 103, 472],\n",
       " [154, 161, 179, 132, 779, 170, 983, 152, 175, 162],\n",
       " [167, 177, 179, 141, 162, 838, 195, 562, 161, 132],\n",
       " [91, 102, 191, 201, 472, 448, 205, 336, 212, 107],\n",
       " [170, 154, 576, 158, 479, 5, 137, 152, 67, 33],\n",
       " [132, 564, 162, 177, 152, 194, 175, 158, 983, 161],\n",
       " [544, 161, 137, 531, 154, 576, 128, 514, 158, 16],\n",
       " [534, 460, 175, 162, 170, 346, 158, 33, 161, 67],\n",
       " [10, 239, 413, 531, 544, 729, 339, 117, 521, 16],\n",
       " [583, 471, 212, 548, 336, 107, 544, 201, 5, 239],\n",
       " [18, 748, 48, 583, 512, 97, 16, 448, 154, 107],\n",
       " [82, 207, 89, 46, 259, 342, 28, 309, 107, 329],\n",
       " [16, 977, 531, 514, 158, 544, 145, 117, 7, 10],\n",
       " [128, 157, 50, 5, 152, 576, 142, 145, 137, 154],\n",
       " [562, 536, 187, 172, 165, 192, 152, 134, 170, 132],\n",
       " [562, 144, 117, 201, 187, 161, 128, 158, 140, 154],\n",
       " [152, 131, 140, 158, 161, 154, 194, 134, 162, 196],\n",
       " [514, 278, 162, 194, 239, 67, 157, 131, 137, 142],\n",
       " [158, 152, 132, 177, 564, 562, 195, 983, 175, 161],\n",
       " [142, 16, 59, 383, 137, 5, 201, 464, 544, 239],\n",
       " [134, 698, 194, 172, 165, 140, 170, 779, 152, 132],\n",
       " [553, 136, 61, 135, 5, 578, 72, 137, 576, 151],\n",
       " [838, 152, 48, 378, 141, 352, 491, 307, 110, 512],\n",
       " [693, 158, 196, 782, 175, 790, 789, 854, 462, 983],\n",
       " [137, 594, 338, 251, 291, 205, 312, 105, 33, 486],\n",
       " [544, 95, 107, 91, 276, 201, 41, 96, 8, 77],\n",
       " [420, 7, 336, 161, 493, 103, 101, 205, 472, 91],\n",
       " [140, 531, 33, 16, 93, 150, 117, 96, 144, 464],\n",
       " [354, 111, 8, 107, 95, 33, 28, 98, 89, 97],\n",
       " [569, 5, 329, 531, 107, 77, 544, 464, 336, 276],\n",
       " [16, 117, 15, 575, 152, 7, 137, 72, 145, 354],\n",
       " [10, 96, 154, 16, 140, 531, 464, 117, 33, 158],\n",
       " [9, 72, 61, 479, 5, 145, 144, 136, 152, 151],\n",
       " [187, 534, 536, 154, 140, 128, 152, 161, 132, 158]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('case1segment_recommendations.pkl', 'rb') as f:\n",
    "    mynewlist = pickle.load(f)\n",
    "mynewlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a09fgGhEMALC"
   },
   "source": [
    "Average epoch time is 90 seconds on Nvidia T4 GPU. Both hit_rate and ndcg values improves initially for first 4 epochs and then converged to a local (or global, I hope) minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq-Tn1PsMXXq"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Congratulations\n",
    "Duration: 2\n",
    "\n",
    "Congratulations! You covered a lot of content and hopefully you have a better understanding of the working of neural matrix factorization model by now.\n",
    "\n",
    "### What we've covered\n",
    "- Create movielens dataset class in Pytorch\n",
    "- Setting the evaluation criteria\n",
    "- Architecture of neural matrix factorization model\n",
    "- Train and evaluating a neural matrix factorization model\n",
    "\n",
    "### Resources\n",
    "- [Colab notebook](https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html)\n",
    "\n",
    "### Next Steps (Learn More)\n",
    "- Notebook based tutorials [here](https://sparsh-ai.github.io/rec-tutorials/)\n",
    "- Read NMF Paper on [Arxiv](https://arxiv.org/abs/1511.06443)\n",
    "- Continue learning by following [this](https://medium.com/@lz2576/a-first-look-at-recommendation-system-with-matrix-factorization-and-neural-nets-7e21e54295c) medium post\n",
    "\n",
    "#### Have a Question?\n",
    "- https://form.jotform.com/211377288388469\n",
    "\n",
    "#### Github Issues\n",
    "- https://github.com/sparsh-ai/reco-tutorials/issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBGWBVTkL_2f"
   },
   "outputs": [],
   "source": [
    "targetss = [1122, 1202, 1500, 1678, 1671, 1661, 107, 62, 1216, 678, 235, 210]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NMF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
