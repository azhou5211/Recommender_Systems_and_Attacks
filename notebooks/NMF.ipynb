{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTHU63hYgwzT"
   },
   "source": [
    "summary: A tutorial to understand the process of building a Neural Matrix Factorization model from scratch in PyTorch on MovieLens-1M dataset.\n",
    "id: neural-matrix-factorization-from-scratch-in-pytorch\n",
    "categories: Pytorch\n",
    "tags: scratch, movielens\n",
    "status: Published \n",
    "authors: Sparsh A.\n",
    "Feedback Link: https://form.jotform.com/211377288388469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC6AkZdChapT"
   },
   "source": [
    "# Neural Matrix Factorization from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-67Oh2k3uCIW"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## What you'll learn\n",
    "Duration: 2\n",
    "\n",
    "- Create movielens dataset class in Pytorch\n",
    "- Setting the evaluation criteria\n",
    "- Architecture of neural matrix factorization model\n",
    "- Train and evaluating a neural matrix factorization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slD2OYQvIfhG"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Dataset\n",
    "Duration: 5\n",
    "\n",
    "After downloading and expanding the movielens-1m dataset, we will create the dataset class as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kEYXkACmIe-B"
   },
   "outputs": [],
   "source": [
    "class Rating_Datset(torch.utils.data.Dataset):\n",
    "    def __init__(self, user_list, item_list, rating_list):\n",
    "        super(Rating_Datset, self).__init__()\n",
    "        self.user_list = user_list\n",
    "        self.item_list = item_list\n",
    "        self.rating_list = rating_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_list[idx]\n",
    "        item = self.item_list[idx]\n",
    "        rating = self.rating_list[idx]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(user, dtype=torch.long),\n",
    "            torch.tensor(item, dtype=torch.long),\n",
    "            torch.tensor(rating, dtype=torch.float)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOyBk8riI66H"
   },
   "source": [
    "The name of our class is *Rating_Dataset* and it is getting inherited from PyTorch *Dataset* base class. The *__getitem__* method is helping us in 2 ways: 1) It is reinforcing the type to [long, long, float] and returning the tensor version of the tuple for the given index id.\n",
    "\n",
    "We are also creating a helper dataset class to put all the data processing functions under a single umbrella. This class contains 5 methods:\n",
    "\n",
    "- *_reindex*: process dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "- *_leave_one_out*: leave-one-out evaluation protocol in paper [https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)\n",
    "- *_negative_sampling*: randomly selects n negative examples for each positive one\n",
    "- *get_train_instance*: merge the examples of train data with negative samples and return the PyTorch dataloader object\n",
    "- *get_test_instance*: merge the examples of test data with negative samples and return the PyTorch dataloader object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Gd05RMI-5i"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Evaluation criteria\n",
    "Duration: 5\n",
    "\n",
    "Next, we are defining evaluation metrics. We are using Hit Rate and NDCG as our evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VVd3dzFVI3Tw"
   },
   "outputs": [],
   "source": [
    "def hit(ng_item, pred_items):\n",
    "    if ng_item in pred_items:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def ndcg(ng_item, pred_items):\n",
    "    if ng_item in pred_items:\n",
    "        index = pred_items.index(ng_item)\n",
    "        return np.reciprocal(np.log2(index+2))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def metrics(model, test_loader, top_k, device):\n",
    "    HR, NDCG = [], []\n",
    "\n",
    "    for user, item, label in test_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "\n",
    "        predictions = model(user, item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommends = torch.take(\n",
    "                item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        ng_item = item[0].item() # leave one-out evaluation has only one item per user\n",
    "        HR.append(hit(ng_item, recommends))\n",
    "        NDCG.append(ndcg(ng_item, recommends))\n",
    "\n",
    "    return np.mean(HR), np.mean(NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieH6c6OKJVrq"
   },
   "source": [
    "The metrics function is first loading the user and item variables to the right device (e.g. to the GPU if it is enabled), then getting predictions from the model and then finally calculating (& returning) the hit_rate_at_k and ndcg_at_k values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzkunJLrJeae"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Defining Model Architectures\n",
    "Duration: 10\n",
    "\n",
    "After defining the dataset class and evaluation function, it is time to define the model architecture.\n",
    "\n",
    "We are going to use *Neural Collaborative Filtering for Personalized Ranking*. This model leverages the flexibility and non-linearity of neural networks to replace dot products of matrix factorization, aiming at enhancing the model expressiveness. In specific, this model is structured with two subnetworks including generalized matrix factorization (GMF) and MLP and models the interactions from two pathways instead of simple inner products. The outputs of these two networks are concatenated for the final prediction scores calculation.\n",
    "\n",
    "![nmf_architecture](img/nmf_architecture.png)\n",
    "\n",
    "In this architecture, we are first creating the user and item embedding layers for both MLP and MF architectures, and with the help of PyTorch ModuleList, we are creating MLP architecture. Then, in the forward method, we are passing user and item indices list in the embedding layers and then concatenating and multiplying the MLP and MF embedding layers respectively. And finally, concatenating the MLP and MF feature layers and a logistic activation at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2Ywo9NU-K7yy"
   },
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, args, num_users, num_items):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num_mf = args.factor_num\n",
    "        self.factor_num_mlp =  int(args.layers[0]/2)\n",
    "        self.layers = args.layers\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n",
    "        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n",
    "\n",
    "        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n",
    "        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n",
    "\n",
    "        for m in self.fc_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.affine_output.weight)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "\n",
    "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "\n",
    "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
    "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "\n",
    "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcyotvWVLiBo"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Training and evaluation\n",
    "Duration: 10\n",
    "\n",
    "We are using following hyperparameters to train the model:\n",
    "- Learning rate is 0.001\n",
    "- Dropout rate is 0.2\n",
    "- Running for 10 epochs\n",
    "- HitRate@10 and NDCG@10\n",
    "- 4 negative samples for each positive one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "MODEL_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", \n",
    "    type=int, \n",
    "    default=42, \n",
    "    help=\"Seed\")\n",
    "parser.add_argument(\"--lr\", \n",
    "    type=float, \n",
    "    default=0.001, \n",
    "    help=\"learning rate\")\n",
    "parser.add_argument(\"--dropout\", \n",
    "    type=float,\n",
    "    default=0.2,  \n",
    "    help=\"dropout rate\")\n",
    "parser.add_argument(\"--batch_size\", \n",
    "    type=int, \n",
    "    default=256, \n",
    "    help=\"batch size for training\")\n",
    "parser.add_argument(\"--epochs\", \n",
    "    type=int,\n",
    "    default=30,  \n",
    "    help=\"training epoches\")\n",
    "parser.add_argument(\"--top_k\", \n",
    "    type=int, \n",
    "    default=10, \n",
    "    help=\"compute metrics@top_k\")\n",
    "parser.add_argument(\"--factor_num\", \n",
    "    type=int,\n",
    "    default=32, \n",
    "    help=\"predictive factors numbers in the model\")\n",
    "parser.add_argument(\"--layers\",\n",
    "    nargs='+', \n",
    "    default=[64,32,16,8],\n",
    "    help=\"MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\")\n",
    "parser.add_argument(\"--num_ng\", \n",
    "    type=int,\n",
    "    default=4, \n",
    "    help=\"Number of negative samples for training set\")\n",
    "parser.add_argument(\"--num_ng_test\", \n",
    "    type=int,\n",
    "    default=100, \n",
    "    help=\"Number of negative samples for test set\")\n",
    "parser.add_argument(\"--out\", \n",
    "    default=True,\n",
    "    #default=False,\n",
    "    help=\"save model or not\")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF_Data(object):\n",
    "    \"\"\"\n",
    "    Construct Dataset for NCF\n",
    "    \"\"\"\n",
    "    def __init__(self, args, ratings):\n",
    "        self.ratings = ratings\n",
    "        self.num_ng = args.num_ng\n",
    "        self.num_ng_test = args.num_ng_test\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.preprocess_ratings = self._reindex(self.ratings)\n",
    "\n",
    "        self.user_pool = set(self.ratings['user_id'].unique())\n",
    "        self.item_pool = set(self.ratings['item_id'].unique())\n",
    "\n",
    "        self.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
    "        self.negatives = self._negative_sampling(self.preprocess_ratings)\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    def _reindex(self, ratings):\n",
    "        \"\"\"\n",
    "        Process dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "        \"\"\"\n",
    "        user_list = list(ratings['user_id'].drop_duplicates())\n",
    "        user2id = {w: i for i, w in enumerate(user_list)}\n",
    "\n",
    "        item_list = list(ratings['item_id'].drop_duplicates())\n",
    "        item2id = {w: i for i, w in enumerate(item_list)}\n",
    "\n",
    "        ratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
    "        ratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
    "        ratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
    "        return ratings\n",
    "\n",
    "    def _leave_one_out(self, ratings):\n",
    "        \"\"\"\n",
    "        leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "        \"\"\"\n",
    "        ratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
    "        test = ratings.loc[ratings['rank_latest'] == 1]\n",
    "        train = ratings.loc[ratings['rank_latest'] > 1]\n",
    "        assert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
    "        return train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
    "\n",
    "\n",
    "\n",
    "    def _negative_sampling(self, ratings):\n",
    "        interact_status = (\n",
    "            ratings.groupby('user_id')['item_id']\n",
    "            .apply(set)\n",
    "            .reset_index()\n",
    "            .rename(columns={'item_id': 'interacted_items'}))\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
    "        return interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
    "\n",
    "    def get_train_instance(self):\n",
    "        users, items, ratings = [], [], []\n",
    "        train_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
    "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
    "        for row in train_ratings.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in range(self.num_ng):\n",
    "                users.append(int(row.user_id))\n",
    "                items.append(int(row.negatives[i]))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def get_test_instance(self):\n",
    "        users, items, ratings = [], [], []\n",
    "        test_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
    "        for row in test_ratings.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in getattr(row, 'negative_samples'):\n",
    "                users.append(int(row.user_id))\n",
    "                items.append(int(i))\n",
    "                ratings.append(float(0))\n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lMKJ2FJrL27U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#args = parser.parse_args(\"\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#writer = SummaryWriter()\n",
    "\n",
    "# seed for Reproducibility\n",
    "seed_everything(seed=42)\n",
    "\n",
    "# load data\n",
    "ml_1m_training = pd.read_csv(\"../data/MovieLens.training\", sep=\"\\t\", names = ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "ml_1m_training\n",
    "\n",
    "ml_1m_test = pd.read_csv(\"../data/MovieLens.test\", sep=\"\\t\", names = ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "ml_1m_test\n",
    "\n",
    "ml_1m = ml_1m_training\n",
    "ml_1m\n",
    "\n",
    "# set the num_users, items\n",
    "num_users = ml_1m['user_id'].nunique()+1\n",
    "num_items = ml_1m['item_id'].nunique()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapse of epoch 001 is: 00: 00: 23\n",
      "HR: 0.542\tNDCG: 0.304\n",
      "The time elapse of epoch 002 is: 00: 00: 25\n",
      "HR: 0.602\tNDCG: 0.340\n",
      "The time elapse of epoch 003 is: 00: 00: 33\n",
      "HR: 0.625\tNDCG: 0.352\n",
      "The time elapse of epoch 004 is: 00: 00: 25\n",
      "HR: 0.626\tNDCG: 0.360\n",
      "The time elapse of epoch 005 is: 00: 00: 19\n",
      "HR: 0.624\tNDCG: 0.361\n",
      "The time elapse of epoch 006 is: 00: 00: 19\n",
      "HR: 0.627\tNDCG: 0.360\n",
      "The time elapse of epoch 007 is: 00: 00: 19\n",
      "HR: 0.622\tNDCG: 0.361\n",
      "The time elapse of epoch 008 is: 00: 00: 19\n",
      "HR: 0.622\tNDCG: 0.352\n",
      "The time elapse of epoch 009 is: 00: 00: 19\n",
      "HR: 0.616\tNDCG: 0.351\n",
      "The time elapse of epoch 010 is: 00: 00: 19\n",
      "HR: 0.604\tNDCG: 0.347\n",
      "The time elapse of epoch 011 is: 00: 00: 20\n",
      "HR: 0.604\tNDCG: 0.344\n",
      "The time elapse of epoch 012 is: 00: 00: 20\n",
      "HR: 0.608\tNDCG: 0.344\n",
      "The time elapse of epoch 013 is: 00: 00: 20\n",
      "HR: 0.591\tNDCG: 0.333\n",
      "The time elapse of epoch 014 is: 00: 00: 25\n",
      "HR: 0.587\tNDCG: 0.335\n",
      "The time elapse of epoch 015 is: 00: 00: 21\n",
      "HR: 0.592\tNDCG: 0.334\n",
      "The time elapse of epoch 016 is: 00: 00: 22\n",
      "HR: 0.593\tNDCG: 0.334\n",
      "The time elapse of epoch 017 is: 00: 00: 25\n",
      "HR: 0.582\tNDCG: 0.329\n",
      "The time elapse of epoch 018 is: 00: 00: 23\n",
      "HR: 0.590\tNDCG: 0.328\n",
      "The time elapse of epoch 019 is: 00: 00: 22\n",
      "HR: 0.577\tNDCG: 0.326\n",
      "The time elapse of epoch 020 is: 00: 00: 24\n",
      "HR: 0.583\tNDCG: 0.322\n",
      "The time elapse of epoch 021 is: 00: 00: 26\n",
      "HR: 0.594\tNDCG: 0.328\n",
      "The time elapse of epoch 022 is: 00: 00: 23\n",
      "HR: 0.585\tNDCG: 0.323\n",
      "The time elapse of epoch 023 is: 00: 00: 24\n",
      "HR: 0.580\tNDCG: 0.319\n",
      "The time elapse of epoch 024 is: 00: 00: 22\n",
      "HR: 0.578\tNDCG: 0.324\n",
      "The time elapse of epoch 025 is: 00: 00: 24\n",
      "HR: 0.574\tNDCG: 0.318\n",
      "The time elapse of epoch 026 is: 00: 00: 24\n",
      "HR: 0.576\tNDCG: 0.316\n",
      "The time elapse of epoch 027 is: 00: 00: 22\n",
      "HR: 0.579\tNDCG: 0.324\n",
      "The time elapse of epoch 028 is: 00: 00: 20\n",
      "HR: 0.576\tNDCG: 0.321\n",
      "The time elapse of epoch 029 is: 00: 00: 20\n",
      "HR: 0.573\tNDCG: 0.317\n",
      "The time elapse of epoch 030 is: 00: 00: 23\n",
      "HR: 0.564\tNDCG: 0.313\n"
     ]
    }
   ],
   "source": [
    "# construct the train and test datasets\n",
    "data = NCF_Data(args, ml_1m)\n",
    "train_loader = data.get_train_instance()\n",
    "test_loader = data.get_test_instance()\n",
    "\n",
    "# set model and loss, optimizer\n",
    "model = NeuMF(args, num_users, num_items)\n",
    "model = model.to(device)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# train, evaluation\n",
    "best_hr = 0\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    model.train() # Enable dropout (if have).\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user, item, label in train_loader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #writer.add_scalar('loss/Train_loss', loss.item(), epoch)\n",
    "\n",
    "    model.eval()\n",
    "    HR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
    "    #writer.add_scalar('Perfomance/HR@10', HR, epoch)\n",
    "    #writer.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
    "            time.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
    "    print(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
    "\n",
    "    if HR > best_hr:\n",
    "        best_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
    "        if args.out:\n",
    "            torch.save(model.state_dict(), \"baseline.pt\")\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try predicting on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_instance(self):\n",
    "        users, items, ratings = [], [], []\n",
    "        test_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
    "        for row in test_ratings.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in getattr(row, 'negative_samples'):\n",
    "                users.append(int(row.user_id))\n",
    "                items.append(int(i))\n",
    "                ratings.append(float(0))\n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=4)\n",
    "    \n",
    "#test_dataloader = DataLoader(test_data, sampler=RandomSampler(test_data), batch_size=1024)\n",
    "\n",
    "model = NeuMF(args, num_users, num_items)\n",
    "model.load_state_dict(torch.load(\"baseline.pt\"))\n",
    "model.eval()\n",
    "\n",
    "for user, item, label in test_dataloader:\n",
    "    user = user.to(device)\n",
    "    item = item.to(device)\n",
    "\n",
    "    predictions = model(user, item)\n",
    "    _, indices = torch.topk(predictions, top_k)\n",
    "    recommends = torch.take(\n",
    "            item, indices).cpu().numpy().tolist()\n",
    "\n",
    "    ng_item = item[0].item() # leave one-out evaluation has only one item per user\n",
    "    HR.append(hit(ng_item, recommends))\n",
    "    NDCG.append(ndcg(ng_item, recommends))\n",
    "\n",
    "'''\n",
    "test_accuracy = []\n",
    "for (x_batch, y_batch) in test_dataloader:\n",
    "    with torch.no_grad():                                        \n",
    "\n",
    "        x_batch.to(device)                                       \n",
    "        y_batch.to(device)\n",
    "        outputs = model(x_batch)                                 \n",
    "\n",
    "        y_pred = torch.argmax(outputs, axis=1)                    \n",
    "        test_accuracy.append(torch.mean((y_pred == y_batch).float()).item())\n",
    "\n",
    "\n",
    "print(\"Test accuracy: %0.2f\" % np.array(test_accuracy).mean())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a09fgGhEMALC"
   },
   "source": [
    "Average epoch time is 90 seconds on Nvidia T4 GPU. Both hit_rate and ndcg values improves initially for first 4 epochs and then converged to a local (or global, I hope) minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq-Tn1PsMXXq"
   },
   "source": [
    "<!-- ------------------------ -->\n",
    "## Congratulations\n",
    "Duration: 2\n",
    "\n",
    "Congratulations! You covered a lot of content and hopefully you have a better understanding of the working of neural matrix factorization model by now.\n",
    "\n",
    "### What we've covered\n",
    "- Create movielens dataset class in Pytorch\n",
    "- Setting the evaluation criteria\n",
    "- Architecture of neural matrix factorization model\n",
    "- Train and evaluating a neural matrix factorization model\n",
    "\n",
    "### Resources\n",
    "- [Colab notebook](https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html)\n",
    "\n",
    "### Next Steps (Learn More)\n",
    "- Notebook based tutorials [here](https://sparsh-ai.github.io/rec-tutorials/)\n",
    "- Read NMF Paper on [Arxiv](https://arxiv.org/abs/1511.06443)\n",
    "- Continue learning by following [this](https://medium.com/@lz2576/a-first-look-at-recommendation-system-with-matrix-factorization-and-neural-nets-7e21e54295c) medium post\n",
    "\n",
    "#### Have a Question?\n",
    "- https://form.jotform.com/211377288388469\n",
    "\n",
    "#### Github Issues\n",
    "- https://github.com/sparsh-ai/reco-tutorials/issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBGWBVTkL_2f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0t1Kc+nRhloSNvqPXwfuK",
   "collapsed_sections": [],
   "mount_file_id": "1_rp4g6-pyprSUlnrWhYy4cDvUKunUIrD",
   "name": "codelabs-neural-matrix-factorization-from-scratch-in-pytorch.ipynb",
   "provenance": [
    {
     "file_id": "1JaNgnfSp-uA1DZzhAI9usSS_rSk3JqfI",
     "timestamp": 1622022444063
    },
    {
     "file_id": "1ARqzPS9EuVZL3Xroqlz_6AqDLUsWAivx",
     "timestamp": 1622018870681
    },
    {
     "file_id": "1shu4X-Zz6pPN8bOBDKrDSbjNym1uy3ml",
     "timestamp": 1621379458916
    },
    {
     "file_id": "1SaddF89o-d87pl1ecWL_xL0TmHAkDwVd",
     "timestamp": 1621377490593
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
