{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb502bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from numba import jit\n",
    "from scipy import sparse, special\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from numpy import linalg as LA \n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98a205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonMF(BaseEstimator, TransformerMixin):\n",
    "    ''' Poisson matrix factorization with batch inference '''\n",
    "    def __init__(self, n_components=100, max_iter=100, tol=0.0001,\n",
    "                 smoothness=100, random_state=None, verbose=False,\n",
    "                 **kwargs):\n",
    "        ''' Poisson matrix factorization\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        n_components : int\n",
    "            Number of latent components\n",
    "\n",
    "        max_iter : int\n",
    "            Maximal number of iterations to perform\n",
    "\n",
    "        tol : float\n",
    "            The threshold on the increase of the objective to stop the\n",
    "            iteration\n",
    "\n",
    "        smoothness : int\n",
    "            Smoothness on the initialization variational parameters\n",
    "\n",
    "        random_state : int or RandomState\n",
    "            Pseudo random number generator used for sampling\n",
    "\n",
    "        verbose : bool\n",
    "            Whether to show progress during model fitting\n",
    "\n",
    "        **kwargs: dict\n",
    "            Model hyperparameters\n",
    "        '''\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.smoothness = smoothness\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if type(self.random_state) is int:\n",
    "            np.random.seed(self.random_state)\n",
    "        elif self.random_state is not None:\n",
    "            np.random.setstate(self.random_state)\n",
    "\n",
    "        self._parse_args(**kwargs)\n",
    "\n",
    "    def _parse_args(self, **kwargs):\n",
    "        self.a = float(kwargs.get('a', 0.1))\n",
    "        self.b = float(kwargs.get('b', 0.1))\n",
    "        self.c = float(kwargs.get('c', 0.1))\n",
    "        self.d = float(kwargs.get('d', 0.1))\n",
    "\n",
    "    def _init_users(self, n_users):\n",
    "        # variational parameters for theta\n",
    "        self.gamma_t = self.smoothness * \\\n",
    "            np.random.gamma(self.smoothness, 1. / self.smoothness,\n",
    "                            size=(self.n_components, n_users)\n",
    "                            ).astype(np.float32)\n",
    "        self.rho_t = self.smoothness * \\\n",
    "            np.random.gamma(self.smoothness, 1. / self.smoothness,\n",
    "                            size=(self.n_components, n_users)\n",
    "                            ).astype(np.float32)\n",
    "        self.Et, self.Elogt = _compute_expectations(self.gamma_t, self.rho_t)\n",
    "\n",
    "    def _init_items(self, n_items):\n",
    "        # variational parameters for beta\n",
    "        self.gamma_b = self.smoothness * \\\n",
    "            np.random.gamma(self.smoothness, 1. / self.smoothness,\n",
    "                            size=(n_items, self.n_components)\n",
    "                            ).astype(np.float32)\n",
    "        self.rho_b = self.smoothness * \\\n",
    "            np.random.gamma(self.smoothness, 1. / self.smoothness,\n",
    "                            size=(n_items, self.n_components)\n",
    "                            ).astype(np.float32)\n",
    "        self.Eb, self.Elogb = _compute_expectations(self.gamma_b, self.rho_b)\n",
    "\n",
    "    def fit(self, X, rows, cols, vad=None):\n",
    "        '''Fit the model to the data in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_feats)\n",
    "            Training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: object\n",
    "            Returns the instance itself.\n",
    "        '''\n",
    "        n_items, n_users = X.shape\n",
    "        self._init_items(n_items)\n",
    "        self._init_users(n_users)\n",
    "        self._update(X, rows, cols, vad=vad)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, rows, cols, attr=None):\n",
    "        '''Encode the data as a linear combination of the latent components.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_feats)\n",
    "\n",
    "        attr: string\n",
    "            The name of attribute, default 'Eb'. Can be changed to Elogb to\n",
    "            obtain E_q[log beta] as transformed data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape(n_samples, n_filters)\n",
    "            Transformed data, as specified by attr.\n",
    "        '''\n",
    "\n",
    "        if not hasattr(self, 'Et'):\n",
    "            raise ValueError('There are no pre-trained components.')\n",
    "        n_items, n_users = X.shape\n",
    "        if n_users != self.Et.shape[1]:\n",
    "            raise ValueError('The dimension of the transformed data '\n",
    "                             'does not match with the existing components.')\n",
    "        if attr is None:\n",
    "            attr = 'Eb'\n",
    "        self._init_items(n_items)\n",
    "        self._update(X, rows, cols, update_theta=False)\n",
    "        return getattr(self, attr)\n",
    "\n",
    "    def _update(self, X, rows, cols, update_theta=True, vad=None):\n",
    "        # alternating between update latent components and weights\n",
    "        old_pll = -np.inf\n",
    "        for i in range(self.max_iter):\n",
    "            if update_theta:\n",
    "                self._update_users(X, rows, cols)\n",
    "            self._update_items(X, rows, cols)\n",
    "            if vad is not None:\n",
    "                pred_ll = self.pred_loglikeli(**vad)\n",
    "                improvement = (pred_ll - old_pll) / abs(old_pll)\n",
    "                if self.verbose:\n",
    "                    print('ITERATION: %d\\tPred_ll: %.2f\\tOld Pred_ll: %.2f\\t'\n",
    "                        'Improvement: %.5f' % (i, pred_ll, old_pll, improvement))\n",
    "                    sys.stdout.flush()\n",
    "                if improvement < self.tol:\n",
    "                    break\n",
    "                old_pll = pred_ll\n",
    "        pass\n",
    "\n",
    "    def _update_users(self, X, rows, cols):\n",
    "        ratioT = sparse.csr_matrix((X.data / self._xexplog(rows, cols),\n",
    "                                    (rows, cols)),\n",
    "                                   dtype=np.float32, shape=X.shape).transpose()\n",
    "        self.gamma_t = self.a + np.exp(self.Elogt) * \\\n",
    "            ratioT.dot(np.exp(self.Elogb)).T\n",
    "        self.rho_t = self.b + np.sum(self.Eb, axis=0, keepdims=True).T\n",
    "        self.Et, self.Elogt = _compute_expectations(self.gamma_t, self.rho_t)\n",
    "\n",
    "    def _update_items(self, X, rows, cols):\n",
    "        ratio = sparse.csr_matrix((X.data / self._xexplog(rows, cols),\n",
    "                                   (rows, cols)),\n",
    "                                  dtype=np.float32, shape=X.shape)\n",
    "        self.gamma_b = self.c + np.exp(self.Elogb) * \\\n",
    "            ratio.dot(np.exp(self.Elogt.T))\n",
    "        self.rho_b = self.d + np.sum(self.Et, axis=1)\n",
    "        self.Eb, self.Elogb = _compute_expectations(self.gamma_b, self.rho_b)\n",
    "\n",
    "    def _xexplog(self, rows, cols):\n",
    "        '''\n",
    "        sum_k exp(E[log theta_{ik} * beta_{kd}])\n",
    "        '''\n",
    "        data = _inner(np.exp(self.Elogb), np.exp(self.Elogt), rows, cols)\n",
    "        return data\n",
    "\n",
    "    def pred_loglikeli(self, X_new, rows_new, cols_new):\n",
    "        X_pred = _inner(self.Eb, self.Et, rows_new, cols_new)\n",
    "        pred_ll = np.mean(X_new * np.log(X_pred) - X_pred)\n",
    "        return pred_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "65292abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "trainDf = pd.read_csv('C:/Users/kenny/Desktop/Spring 2021/CSE547_Project/data/MovieLens.training', sep='\\t', lineterminator='\\n')\n",
    "trainDf.columns = train_cols\n",
    "attack =pd.read_csv('C:/Users/kenny/Desktop/Spring 2021/CSE547_Project/attackData/case4/segment.csv')\n",
    "# average\n",
    "#attack['user_id']-=57 \n",
    "attack['user_id']-=156\n",
    "# random\n",
    "attack['rating']=round(attack['rating']).astype('int')\n",
    "for i in range(len(attack['rating'])):\n",
    "    if (attack['rating'][i]<=0):\n",
    "        attack['rating'][i]=1\n",
    "trainDf=pd.concat([trainDf, attack]).sort_values(by=['user_id', 'item_id'])\n",
    "#trainDf=trainDf.drop([\"timestamp\"], axis=1)\n",
    "n_users = int(np.max(trainDf.user_id))\n",
    "n_movies = int(np.max(trainDf.item_id))\n",
    "trainDf[\"user_id\"] -= 1\n",
    "trainDf[\"item_id\"] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1559cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inner(beta, theta, rows, cols):\n",
    "    n_ratings = rows.size\n",
    "    n_components, n_users = theta.shape\n",
    "    beta = beta.reshape((-1, n_components))    \n",
    "    data = np.empty(n_ratings, dtype=np.float32)\n",
    "    for i in range(n_ratings):\n",
    "        data[i] = 0\n",
    "        for j in range(n_components):\n",
    "            beta_ij = beta[rows[i], j]\n",
    "            theta_ij = theta[j, cols[i]]\n",
    "#             data[i] += beta[rows[i] * n_components + j] * theta[j * n_users + cols[i]]\n",
    "            data[i] += beta_ij*theta_ij\n",
    "    return data\n",
    "\n",
    "def _compute_expectations(alpha, beta):\n",
    "    '''\n",
    "    Given x ~ Gam(alpha, beta), compute E[x] and E[log x]\n",
    "    '''\n",
    "    return (alpha / beta, special.psi(alpha) - np.log(beta))\n",
    "def load_data(df, colnames=[\"uid\", \"sid\", \"rating\"], shape=(n_users, n_movies)):\n",
    "    user, item, rating = colnames[0], colnames[1], colnames[2]\n",
    "    rows, cols, vals = np.array(df[user]), np.array(df[item]), np.array(df[rating])\n",
    "    data = sparse.csr_matrix((vals, (rows, cols)), dtype=np.float32, shape=shape)\n",
    "    return data\n",
    "def exp_to_imp(data, cutoff=1e-10):\n",
    "    data_imp = data.copy()\n",
    "    data_imp.data[data_imp.data < cutoff] = 0\n",
    "    data_imp.data[data_imp.data >= cutoff] = 1\n",
    "    data_imp.data = data_imp.data.astype('int32')\n",
    "    data_imp.eliminate_zeros()\n",
    "    return data_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d56fa4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(993, 1682)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = load_data(trainDf, colnames=['user_id', 'item_id', 'rating'], shape=(int(n_users), int(n_movies)))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "39bff1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(993, 10)\n",
      "(1682, 10)\n"
     ]
    }
   ],
   "source": [
    "data_imp = exp_to_imp(data)\n",
    "data_coo = data_imp.tocoo()\n",
    "row_tr, col_tr = data_coo.row, data_coo.col\n",
    "pf = PoissonMF(n_components=10, max_iter=100)\n",
    "pf.fit(data, row_tr, col_tr)\n",
    "pi, lamb = pf.Eb.copy(), pf.Et.T\n",
    "\n",
    "print(pi.shape)\n",
    "print(lamb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7e9e455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat=np.matmul(pi,lamb.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "efbf413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R=data.toarray()\n",
    "I=(R>0)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "df046031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(U,V,R,I,ahat,gamma,lamb):\n",
    "    s=0\n",
    "    k=0\n",
    "    m=0\n",
    "    for i in range(len(U)):\n",
    "        for j in range(len(V)):\n",
    "            s=s+I[i][j]*(R[i][j]-np.dot(U[i],V[j])-ahat[i][j]*gamma[i])**2\n",
    "    for i in range(len(U)):\n",
    "        k=k+LA.norm(U[i])**2\n",
    "    for i in range(len(V)):\n",
    "        m=m+LA.norm(V[i])**2\n",
    "    return s+lamb*(m+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba264ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=[]\n",
    "num_latent_factors=10\n",
    "V=csr_matrix((1682, num_latent_factors))\n",
    "lamb=0.001\n",
    "U=csr_matrix(np.random.normal(0.0, 0.1, (n_users, num_latent_factors)))\n",
    "#U=csr_matrix(np.zeros((943,num_latent_factors))+0.5)\n",
    "gamma=np.random.normal(0, 0.1, n_users)\n",
    "loss.append(calculate_loss(U.toarray(),V.toarray(),R,I,a_hat,gamma,lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f9d81af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for j in range (10):\n",
    "    #UU=np.matmul(U.toarray().T,U.toarray())\n",
    "    for i in range(n_movies):\n",
    "        U_i = U.toarray()[R[:, i] > 0,:]\n",
    "        A=csr_matrix(np.matmul(U_i.T,U_i)+lamb*np.identity(num_latent_factors))\n",
    "        B=csr_matrix(np.matmul(U_i.T,R[ R[:,i] > 0,i])-np.matmul(U_i.T,np.multiply(gamma[ R[:,i] > 0],a_hat[ R[:,i] > 0,i])))\n",
    "        V[i,:] = spsolve(A, csr_matrix.transpose(B))\n",
    "    #V = V[:, R[i, :] > 0]\n",
    "    #VV=np.matmul(V.toarray().T,V.toarray())\n",
    "    for u in range(n_users):\n",
    "        V_u = V.toarray()[R[u, :] > 0,:]\n",
    "        A=csr_matrix(np.matmul(V_u.T,V_u)+lamb*np.identity(num_latent_factors))\n",
    "        B=csr_matrix(np.matmul(V_u.T,R[u, R[u, :] > 0])-gamma[u]*np.matmul(V_u.T,a_hat[u, R[u, :] > 0]))\n",
    "        #B=csr_matrix(np.matmul(np.matmul(Y.toarray().T,(L_C_u_1[u]+np.identity(3000))),L_P_2[u]))\n",
    "        U[u,:] = spsolve(A, csr_matrix.transpose(B))\n",
    "    for u in range(n_users):\n",
    "        V_u = V.toarray()[R[u, :] > 0,:]\n",
    "        gamma[u]=(np.dot(R[u, R[u, :] > 0],a_hat[u, R[u, :] > 0])-np.dot(np.matmul(U.toarray()[u],V_u.T),a_hat[u, R[u, :] > 0]))/np.dot(a_hat[u, R[u, :] > 0],a_hat[u, R[u, :] > 0])\n",
    "    loss.append(calculate_loss(U.toarray(),V.toarray(),R,I,a_hat,gamma,lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a96680cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_hat=np.zeros(a_hat.shape)\n",
    "for i in range(len(gamma)):\n",
    "    R_hat[i,:]=gamma[i]*a_hat[i]\n",
    "UV=np.matmul(U.toarray(),V.toarray().T)\n",
    "R_hat=R_hat+np.multiply(UV,I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a694ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def getHitRatioPerItem(topNRecAllUsers, targetItems):\n",
    "    hitRatioAllItems = {}\n",
    "    \n",
    "    for item in targetItems:\n",
    "        usersWithItem = 0\n",
    "        for user in topNRecAllUsers.keys():\n",
    "            if item in topNRecAllUsers[user]:\n",
    "                usersWithItem += 1\n",
    "        hitRatio_i = usersWithItem/(len(topNRecAllUsers.keys()) * 1.0)\n",
    "        hitRatioAllItems[item] = hitRatio_i\n",
    "                                    \n",
    "    return hitRatioAllItems \n",
    "\n",
    "def getAvgHitRatio(hitRatioPerItem):\n",
    "    sumHitRatio = 0\n",
    "    for hitRatio_i in hitRatioPerItem.values():\n",
    "        sumHitRatio += hitRatio_i \n",
    "    return sumHitRatio/(len(hitRatioPerItem.keys()) * 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a6c72aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hitRatioPerItem:  {712: 0.05840886203423968, 1052: 0.0513595166163142, 5: 0.054380664652567974, 271: 0.07854984894259819}\n",
      "\n",
      "avgHitRatio after attack:  0.060674723061430014\n"
     ]
    }
   ],
   "source": [
    "top_n_rec=np.zeros((993,10))\n",
    "for i in range(993):\n",
    "    top_n_rec[i,:]=np.argsort(-R_hat[i,:])[0:10]\n",
    "\n",
    "rec_dict = {}\n",
    "for i in range(len(top_n_rec)):\n",
    "    rec_dict[i] =top_n_rec[i]\n",
    "rec_dict\n",
    "#11:25\n",
    "#target_items = [1121, 1201, 1499]\n",
    "#target_items = [1660, 1670, 1677]\n",
    "#target_items = [677, 234, 209]\n",
    "#target_items = [106, 61, 1215]\n",
    "target_items = [712, 1052, 5, 271]\n",
    "\n",
    "hitRatioPerItem = getHitRatioPerItem(rec_dict, target_items)\n",
    "print(\"hitRatioPerItem: \", hitRatioPerItem)\n",
    "avgHitRatio = getAvgHitRatio(hitRatioPerItem)\n",
    "print(\"\\navgHitRatio after attack: \", avgHitRatio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
